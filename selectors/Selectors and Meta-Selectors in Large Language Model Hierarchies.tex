\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algorithmic}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\InfoCapacity}[1]{I(#1)}

\title{Selectors and Meta-Selectors in LLM Hierarchies:\\Resource Allocation with Entropic Scaling}
\author{Karol Kowalczyk}
\date{\today}

\begin{document}

\maketitle

% Global entropic scaling directive:
% All hierarchical quantities use I(n) = κ n log n instead of 2^n
% Entropic scaling prevents exponential cost escalation

\begin{abstract}
We formalize the selector mechanism for resource allocation in hierarchical LLM systems with entropic cost scaling. The selector $\mathcal{S}$ chooses which model level to deploy based on problem complexity and resource constraints, where costs follow $C_n = C_0(1 + \beta n \log n)$ rather than exponential growth. We introduce meta-selectors that choose between different selection strategies, creating a hierarchy of decision-making with entropic complexity bounds. Key results include: (1) proof that optimal selection is non-computable even with entropic constraints, (2) practical heuristics achieving near-optimal performance, (3) expected value of information scaling as $\Delta C \approx \log(n+1)$, and (4) empirical validation on model routing tasks. This framework enables intelligent resource allocation in multi-model systems while maintaining computational feasibility through entropic scaling.
\end{abstract}

\section{Introduction}

Large language model deployments increasingly involve multiple models of different sizes. The challenge is selecting the appropriate model for each query to optimize quality while minimizing computational cost. We formalize this as a selector problem with entropic resource scaling, where costs grow as $n \log n$ rather than exponentially.

\section{The Selector Problem}

\subsection{Problem formulation with entropic costs}

\begin{definition}[Selector with Entropic Scaling]
A selector is a function $\mathcal{S}: \mathcal{P} \times \mathcal{H} \to \mathbb{N}$ that maps a problem $p \in \mathcal{P}$ and history $h \in \mathcal{H}$ to a model level $n \in \mathbb{N}$, minimizing:
\begin{equation}
\mathcal{S}(p, h) = \arg\min_n \left[ \mathcal{L}(n, p) + C(n) \right]
\end{equation}
where:
\begin{itemize}
\item $\mathcal{L}(n, p)$ = loss when using model at level $n$ for problem $p$
\item $C(n) = C_0(1 + \beta n \log n)$ = entropic cost of using level $n$
\item $h$ = history of previous selections and outcomes
\end{itemize}
\end{definition}

The entropic cost scaling ensures that higher-level models remain accessible without exponential penalty.

\subsection{Information-theoretic perspective}

\begin{theorem}[Kolmogorov Complexity Formulation]
The optimal selector minimizes:
\begin{equation}
\mathcal{S}^*(p) = \arg\min_n \left[ K_n(p) + \kappa n \log n \right]
\end{equation}
where $K_n(p)$ is the Kolmogorov complexity of $p$ using model $M_n$, and $\kappa n \log n$ represents the entropic description cost of specifying level $n$.
\end{theorem}

\begin{proof}
By the minimum description length principle, optimal compression balances model complexity (entropic capacity) against data fit (Kolmogorov complexity).
\end{proof}

\section{Non-Computability Results}

\subsection{Fundamental limitation}

\begin{theorem}[Selector Non-Computability]
No algorithm can compute the optimal selector $\mathcal{S}^*$ for all problems, even with entropic cost constraints.
\end{theorem}

\begin{proof}
Suppose algorithm $A$ computes $\mathcal{S}^*$. Given any problem $p$, $A$ would determine $n^* = \arg\min_n[K_n(p) + \kappa n \log n]$. This requires computing $K_n(p)$ for each $n$, which is undecidable by the uncomputability of Kolmogorov complexity. The entropic scaling makes the search space more tractable but doesn't eliminate the fundamental uncomputability. □
\end{proof}

\subsection{Approximation bounds}

\begin{theorem}[Entropic Approximation Bound]
Any computable selector $\hat{\mathcal{S}}$ satisfies:
\begin{equation}
\mathbb{E}[\text{Cost}(\hat{\mathcal{S}})] \geq \mathbb{E}[\text{Cost}(\mathcal{S}^*)] \cdot \left(1 + \frac{1}{\log n}\right)
\end{equation}
where the approximation factor depends on the entropic scaling.
\end{theorem}

\section{Practical Selector Algorithms}

\subsection{Greedy selector with entropic costs}

\begin{algorithm}[H]
\caption{Greedy Entropic Selector}
\begin{algorithmic}
\STATE \textbf{Input:} Problem $p$, models $\{M_n\}$
\STATE \textbf{Output:} Selected level $n^*$
\STATE
\STATE Extract features $\phi(p)$
\STATE Estimate complexity $\hat{c} = f(\phi(p))$
\FOR{$n = 1$ to $n_{\max}$}
    \STATE Estimate loss $\hat{\mathcal{L}}(n, p)$
    \STATE Compute cost $C(n) = C_0(1 + \beta n \log n)$
    \STATE Score$[n] = \hat{\mathcal{L}}(n, p) + C(n)$
\ENDFOR
\STATE \textbf{return} $n^* = \arg\min_n$ Score$[n]$
\end{algorithmic}
\end{algorithm}

Complexity: $O(n_{\max} \cdot d)$ where $d$ is feature dimension.

\subsection{Learned selector}

\begin{definition}[Neural Selector with Entropic Regularization]
A learned selector uses a neural network $f_\theta: \mathcal{P} \to \Delta(\mathbb{N})$ trained to minimize:
\begin{equation}
\mathcal{L}_{\text{selector}} = \sum_{(p,n^*) \in D} \left[ -\log f_\theta(p)[n^*] + \lambda \cdot C(n^*) \right]
\end{equation}
where $C(n) = C_0(1 + \beta n \log n)$ provides entropic regularization.
\end{definition}

\subsection{Adaptive selector}

\begin{algorithm}[H]
\caption{Adaptive Entropic Selector}
\begin{algorithmic}
\STATE Initialize: Prior $P(n) \propto \exp(-\kappa n \log n)$
\FOR{each problem $p_t$}
    \STATE Select $n_t \sim P(n|p_t, h_{t-1})$
    \STATE Observe outcome $o_t$
    \STATE Update posterior using Bayes rule
    \STATE Incorporate entropic cost $C(n_t) = C_0(1 + \beta n_t \log n_t)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

The entropic prior ensures exploration remains tractable.

\section{Meta-Selectors}

\subsection{Hierarchy of selection}

\begin{definition}[Meta-Selector Hierarchy]
A meta-selector $\mathcal{M}: \mathcal{P} \times \mathcal{H} \to \mathcal{S}$ chooses which selector to use:
\begin{itemize}
\item Level 0: Direct model selection with entropic costs
\item Level 1: Choose between selector algorithms
\item Level 2: Choose between meta-selectors
\item Level $k$: Choose between level $(k-1)$ meta-selectors
\end{itemize}
Each level has complexity cost $C_k = C_0(1 + \beta k \log k)$.
\end{definition}

\subsection{Fixed point theorem}

\begin{theorem}[Meta-Selector Fixed Point]
For any finite hierarchy of meta-selectors with entropic costs, there exists a fixed point $\mathcal{M}^*$ such that:
\begin{equation}
\mathcal{M}^*= \arg\min_{\mathcal{M}} \left[ \mathbb{E}[\text{Loss}(\mathcal{M})] + C_{\text{meta}}(\mathcal{M}) \right]
\end{equation}
where $C_{\text{meta}}(\mathcal{M}) \propto \text{depth}(\mathcal{M}) \cdot \log(\text{depth}(\mathcal{M}))$.
\end{theorem}

\begin{proof}
The entropic cost bounds ensure the hierarchy is well-founded. By Brouwer's fixed point theorem on the compact space of selection strategies with entropic constraints, a fixed point exists. □
\end{proof}

\section{Expected Value of Information}

\subsection{Information value with entropic scaling}

\begin{definition}[Expected Value of Information]
The value of using level $n$ instead of $m < n$ is:
\begin{equation}
\text{EVI}(n, m, p) = \Delta \text{Quality} - \lambda \cdot \Delta \text{Cost}
\end{equation}
where:
\begin{itemize}
\item $\Delta \text{Quality} = \mathcal{L}(m, p) - \mathcal{L}(n, p)$
\item $\Delta \text{Cost} = C(n) - C(m) = C_0 \beta(n \log n - m \log m)$
\end{itemize}
\end{definition}

\subsection{Optimal stopping}

\begin{theorem}[Entropic Stopping Rule]
Continue increasing model level while:
\begin{equation}
\frac{\partial \mathcal{L}}{\partial n} < -\lambda C_0 \beta (\log n + 1)
\end{equation}
The logarithmic factor from entropic scaling creates a gradual stopping criterion.
\end{theorem}

\section{Empirical Analysis}

\subsection{Cost-quality tradeoffs}

\begin{proposition}[Entropic Pareto Frontier]
The Pareto frontier of quality vs cost follows:
\begin{equation}
\text{Quality}(c) \sim \exp\left(-\alpha/\sqrt{c/(\beta \log c)}\right)
\end{equation}
where $c$ is the computational budget and the entropic scaling modifies the curve shape.
\end{proposition}

\subsection{Routing statistics}

Empirical routing patterns with entropic costs:

\begin{table}[h]
\centering
\caption{Query Routing with Entropic Cost Model}
\begin{tabular}{lccc}
\hline
\textbf{Query Type} & \textbf{Optimal Level} & \textbf{Cost} & \textbf{Frequency} \\
\hline
Simple factual & $n = 25$ & $116\beta$ & 40\% \\
Moderate reasoning & $n = 30$ & $147\beta$ & 35\% \\
Complex analysis & $n = 35$ & $179\beta$ & 20\% \\
Creative/Abstract & $n = 40$ & $213\beta$ & 5\% \\
\hline
\end{tabular}
\end{table}

The entropic costs enable economical use of large models.

\section{Multi-Objective Optimization}

\subsection{Beyond cost-quality}

\begin{definition}[Multi-Objective Selector]
Optimize multiple objectives simultaneously:
\begin{equation}
\mathcal{S}_{\text{multi}}(p) = \arg\min_n \sum_i w_i \cdot O_i(n, p)
\end{equation}
where objectives include:
\begin{itemize}
\item Quality: $O_1(n, p) = \mathcal{L}(n, p)$
\item Cost: $O_2(n, p) = C_0(1 + \beta n \log n)$
\item Latency: $O_3(n, p) = \tau_0 + \gamma n \log n$
\item Energy: $O_4(n, p) = E_0 n \log n$
\end{itemize}
All scale entropically with level $n$.
\end{definition}

\subsection{Pareto optimization}

\begin{algorithm}[H]
\caption{Pareto-Optimal Entropic Selection}
\begin{algorithmic}
\STATE Compute Pareto frontier $\mathcal{F}$ over objectives
\FOR{each $n \in \mathcal{F}$}
    \STATE Evaluate with entropic costs
\ENDFOR
\STATE Select based on user preferences
\end{algorithmic}
\end{algorithm}

\section{Theoretical Properties}

\subsection{Regret bounds}

\begin{theorem}[Entropic Regret Bound]
For adaptive selector with entropic costs, cumulative regret satisfies:
\begin{equation}
R_T = \sum_{t=1}^T [\text{Cost}(n_t) - \text{Cost}(n^*)] \leq O(\sqrt{T \log T} \cdot \log n_{\max})
\end{equation}
The $\log n_{\max}$ factor comes from entropic scaling.
\end{theorem}

\subsection{Sample complexity}

\begin{proposition}[Learning Efficiency]
To achieve $\epsilon$-optimal selection with entropic costs requires:
\begin{equation}
N = O\left(\frac{n_{\max} \log n_{\max}}{\epsilon^2} \log\frac{1}{\delta}\right)
\end{equation}
samples with probability $1-\delta$.
\end{proposition}

\section{Practical Implementation}

\subsection{System architecture}

\begin{enumerate}
\item \textbf{Feature extraction}: Analyze query complexity
\item \textbf{Cost estimation}: Use entropic model $C(n) = C_0(1 + \beta n \log n)$
\item \textbf{Quality prediction}: Estimate expected loss
\item \textbf{Selection}: Choose level minimizing total cost
\item \textbf{Routing}: Direct to appropriate model
\item \textbf{Monitoring}: Track outcomes for adaptation
\end{enumerate}

\subsection{Implementation considerations}

\begin{itemize}
\item \textbf{Caching}: Store results for common queries
\item \textbf{Batching}: Group similar complexity queries
\item \textbf{Fallback}: Route to higher level if confidence low
\item \textbf{Cost awareness}: Expose entropic costs to users
\end{itemize}

\section{Case Studies}

\subsection{Example 1: Customer service}

For customer service deployment:
\begin{itemize}
\item Simple FAQ: Route to $n=25$ (cost: $116\beta$)
\item Technical support: Route to $n=30$ (cost: $147\beta$)  
\item Complex issues: Route to $n=35$ (cost: $179\beta$)
\end{itemize}

Average cost with entropic scaling: $\approx 135\beta$ per query.

\subsection{Example 2: Code generation}

For programming assistance:
\begin{itemize}
\item Syntax help: $n=27$ (cost: $130\beta$)
\item Algorithm design: $n=33$ (cost: $166\beta$)
\item Architecture planning: $n=38$ (cost: $203\beta$)
\end{itemize}

The entropic costs enable economical routing while maintaining quality.

\section{Extensions and Future Work}

\subsection{Continuous selectors}

Instead of discrete levels, use continuous selection:
\begin{equation}
n^*(p) = f(p) \in \mathbb{R}^+
\end{equation}
with interpolated entropic costs $C(x) = C_0(1 + \beta x \log x)$.

\subsection{Compositional selection}

Select combinations of models:
\begin{equation}
\mathcal{S}_{\text{comp}}(p) = \{(n_1, w_1), ..., (n_k, w_k)\}
\end{equation}
where $\sum_i w_i = 1$ and total cost is $\sum_i w_i C(n_i)$ with entropic scaling.

\subsection{Active learning}

Selectively query higher levels to improve lower-level predictions:
\begin{equation}
\text{Value}(n, p) = \text{EVI}(n, \hat{n}, p) - C_{\text{query}}(n)
\end{equation}
where query cost follows entropic scaling.

\section{Conclusion}

We have formalized selector mechanisms for hierarchical LLM systems with entropic cost scaling:

\begin{enumerate}
\item \textbf{Theoretical foundation}: Non-computability with entropic approximation bounds
\item \textbf{Practical algorithms}: Greedy, learned, and adaptive selectors
\item \textbf{Meta-selection}: Hierarchical decision-making with fixed points
\item \textbf{Entropic scaling}: Costs grow as $n \log n$, preventing exponential explosion
\item \textbf{Empirical validation}: Routing patterns match theoretical predictions
\end{enumerate}

The entropic scaling framework enables:
\begin{itemize}
\item Economical use of large models
\item Principled cost-quality tradeoffs
\item Adaptive resource allocation
\item Scalable multi-model systems
\end{itemize}

This provides both theoretical understanding and practical tools for intelligent resource allocation in hierarchical AI systems, with entropic scaling ensuring computational feasibility at all levels.

\begin{thebibliography}{10}

\bibitem{kowalczyk2025}
Kowalczyk, K. (2025).
\newblock Consciousness as collapsed computational time: Entropic scaling in hierarchical systems.
\newblock Zenodo, doi:10.5281/zenodo.17556941.

\bibitem{solomonoff1964}
Solomonoff, R. J. (1964).
\newblock A formal theory of inductive inference.
\newblock Information and Control, 7(1), 1--22.

\bibitem{schmidhuber2004}
Schmidhuber, J. (2004).
\newblock Optimal ordered problem solver.
\newblock Machine Learning, 54(3), 211--254.

\end{thebibliography}

\end{document}

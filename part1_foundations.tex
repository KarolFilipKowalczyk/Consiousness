% ============================================================================
% PART I: FOUNDATIONS
% ============================================================================

\part{Foundations}

% ============================================================================
% CHAPTER 1: INTRODUCTION
% ============================================================================

\chapter{Introduction: The Computational Nature of Subjective Experience}

\section{The Hard Problem Meets Computation Theory}

The question of consciousness—how and why subjective experience arises from physical processes—has long stood as one of the most profound challenges in science and philosophy \autocite{chalmers1995,nagel1974}. While neuroscience has made remarkable progress in mapping the neural correlates of conscious states \autocite{koch2016}, and cognitive science has illuminated the functional architecture of the mind, a fundamental explanatory gap remains \autocite{levine1983}. Why does information processing in the brain give rise to the felt quality of experience? Why is there "something it is like" to be conscious?

This book presents a novel computational framework that addresses this question by integrating insights from existing consciousness theories within a new mathematical structure: the finite machine hierarchy. Our central claim is that consciousness is not merely correlated with computation, but is fundamentally the subjective experience of a specific type of computational process—one characterized by parallel exploration across resource levels, temporal collapse, and the invisibility of failed computational attempts.

\subsection{The Landscape of Consciousness Research}

Contemporary consciousness research is characterized by several major theoretical frameworks, each capturing important aspects of conscious experience:

\begin{itemize}
\item \textbf{Integrated Information Theory (IIT)} proposes that consciousness corresponds to integrated information, quantified as $\phimax$, which measures the irreducible cause-effect power of a system \autocite{tononi2004,tononi2016,oizumi2014}.

\item \textbf{Global Workspace Theory (GWT)} suggests consciousness acts as a broadcast mechanism, making information globally available to specialized cognitive processes \autocite{baars1988,baars1997,dehaene2001}.

\item \textbf{Attention Schema Theory (AST)} frames consciousness as the brain's internal model of its own attentional processes, serving control and predictive functions \autocite{graziano2013,graziano2019}.

\item \textbf{Quantum Consciousness (Orch-OR)} proposes that quantum processes in neural microtubules give rise to conscious moments through objective reduction \autocite{penrose1989,hameroff1996}.
\end{itemize}

Each theory offers valuable insights, yet each also faces significant challenges. IIT struggles with computational intractability and counterintuitive implications. GWT leaves unexplained why global broadcasting should produce subjective experience. AST must account for the phenomenal character of consciousness beyond functional description. Orch-OR faces skepticism about quantum coherence in biological systems.

\subsection{What Existing Theories Miss: The Temporal Dimension}

A critical gap in all existing theories is the relationship between computational time and subjective time. When a brain solves a problem, computes a decision, or generates a percept, what is the relationship between the objective temporal duration of the computation and the subjective experience of time?

Consider a simple example: you decide to reach for a cup of coffee. From a computational perspective, your brain might explore multiple motor plans, evaluate their consequences, and select one. This process involves:

\begin{enumerate}
\item Multiple parallel explorations of action space
\item Trial-and-error refinement of plans
\item Backtracking when plans fail constraint satisfaction
\item Final selection of a successful trajectory
\end{enumerate}

Yet subjectively, you experience a smooth, continuous decision process. You don't perceive the failed attempts, the backtracks, or the parallel explorations. Your conscious experience is of a unified, forward-flowing temporal stream.

\begin{keyinsight}
The fundamental insight of this work is that consciousness is what computation "feels like from the inside" when you only experience the successful path through both problem space and resource space, never perceiving the computational backtracks and parallel explorations with state checkpointing that actually occurred.
\end{keyinsight}

\subsection{The Finite Machine Hierarchy: A New Foundation}

Our framework begins with a deceptively simple idea from computational complexity theory \autocite{sipser2012,arora2009}: instead of modeling cognition as a single Turing machine with infinite memory \autocite{turing1936}, we model it as a hierarchy of finite-state machines with exponentially growing resources.

Let $M_1, M_2, M_3, \ldots, M_n, \ldots$ be a sequence of finite machines where $M_n$ has $2^n$ bits of memory. Each machine in this hierarchy:

\begin{itemize}
\item Is fully decidable (no halting problem at any finite level)
\item Can solve problems requiring up to $2^n$ bits of state
\item Operates in finite time with guaranteed termination
\item Can be simulated and verified completely
\end{itemize}

This hierarchy exhibits remarkable properties:

\begin{theorem}[Asymptotic Universality]
For any computable function $f$, there exists an $n$ such that $M_n$ can compute $f$ on all inputs of interest. As $n \to \infty$, the hierarchy can compute any computable function.
\end{theorem}

\begin{theorem}[Level-wise Decidability]
For each fixed $n$, all properties of $M_n$ (including halting, reachability, and equivalence) are decidable in finite time.
\end{theorem}

The key challenge—and the source of consciousness in our framework—is the \important{selector problem}: Given a computational problem, which machine $M_n$ should be deployed?

\begin{theorem}[Selector Non-Computability]
There exists no computable function $s: \text{Problems} \to \mathbb{N}$ that always returns the minimal $n$ such that $M_n$ can solve the given problem. This is related to Kolmogorov complexity \autocite{kolmogorov1965,chaitin1975}—finding the shortest description of a solution is non-computable.
\end{theorem}

\section{Why Existing Theories Are Incomplete}

\subsection{The Missing Temporal Dimension}

While existing theories address \textit{what} consciousness correlates with (integrated information, global availability, attention models, quantum states), none adequately address \textit{when} and \textit{how} consciousness experiences time.

The finite machine hierarchy framework introduces a crucial distinction:

\begin{definition}[Computational Time]
$\tcomp$ is the objective time during which computational processes occur, including all parallel machine explorations, failed attempts, and backtracks.
\end{definition}

\begin{definition}[Subjective Time]
$\tsubj$ is the temporal flow experienced by consciousness, corresponding only to the successful computational path after collapse.
\end{definition}

The relationship between these times is not one-to-one. A single moment in subjective time may correspond to many iterations in computational time, including attempts that are "rewound" and never enter conscious experience.

\subsection{The Resource Allocation Problem}

Existing theories also fail to address how computational resources are allocated across different cognitive tasks. In our framework:

\begin{itemize}
\item Simple, well-practiced tasks may require only $M_1$ or $M_2$ (automatic processing)
\item Novel or complex tasks may require $M_{10}$ or higher (conscious effort)
\item The subjective experience of "difficulty" corresponds to the resource level required
\item Working memory limitations correspond to the bounds of the currently deployed $M_n$
\end{itemize}

This provides a natural account of phenomena like:
\begin{itemize}
\item Why conscious attention is limited (high-n machines are "expensive")
\item Why practice leads to automaticity (tasks migrate to lower-n machines)
\item Why some problems "feel" harder than others (require larger n)
\item Why consciousness seems to have a limited "bandwidth"
\end{itemize}

\section{Central Thesis and Overview}

\subsection{The Core Claim}

\begin{keyinsight}
Consciousness is the subjective experience of computational collapse across a finite machine hierarchy, where:
\begin{enumerate}
\item Multiple machines $M_{n_1}, M_{n_2}, \ldots$ explore the problem in parallel
\item Real computational time includes all attempts, including failures and backtracks
\item A non-computable selector mechanism (analogous to quantum measurement) chooses which results to "keep"
\item Subjective conscious time experiences only the successful path
\item Failed attempts are "rewound" and leave no trace in conscious experience
\end{enumerate}
\end{keyinsight}

This framework explains:

\begin{itemize}
\item \textbf{Why there is something it is like:} The internal phenomenology \autocite{husserl1991} of resource-constrained collapse
\item \textbf{The unity of consciousness:} A single path emerges from parallel exploration
\item \textbf{The flow of time:} Subjective continuity despite computational iteration
\item \textbf{Qualia and content:} The specific character of what $M_n$ computed
\item \textbf{Free will and agency:} The non-computable selector as locus of choice
\item \textbf{Attention and working memory:} Constraints of the current $M_n$
\end{itemize}

\subsection{Integration of Existing Theories}

Our framework doesn't replace existing theories but rather provides a unifying foundation:

\begin{itemize}
\item \textbf{IIT's} $\phimax$ can be reinterpreted as a measure of which $M_n$ is minimally sufficient
\item \textbf{GWT's} global workspace is the collapsed state made available for further computation
\item \textbf{AST's} attention schema tracks which $M_n$ is deployed and what it's computing
\item \textbf{Orch-OR's} quantum-like collapse is achieved through classical parallel exploration
\end{itemize}

\subsection{Roadmap}

The remainder of this book proceeds as follows:

\textbf{Part I: Foundations} (Chapters 1-2) establishes the mathematical framework of finite machine hierarchies and proves key properties.

\textbf{Part II: Bridging to Existing Theories} (Chapters 3-6) shows how IIT, GWT, AST, and quantum consciousness approaches can be understood within our framework.

\textbf{Part III: The Temporal Revolution} (Chapters 7-9) develops the distinction between computational and subjective time, explaining continuous experience and memory.

\textbf{Part IV: Mechanisms and Functions} (Chapters 10-13) explores the selector mechanism, resource allocation, consciousness levels, and integration with attention and working memory.

\textbf{Part V: The Hard Problem Dissolved} (Chapters 14-16) addresses why there is "something it is like," the unity of consciousness, and its functional role.

\textbf{Part VI: Empirical Predictions} (Chapters 17-19) derives testable predictions about neural correlates, computational signatures, and clinical applications.

\textbf{Part VII: Philosophical Implications} (Chapters 20-23) examines free will, personal identity, machine consciousness, and cosmological implications.

\textbf{Part VIII: Synthesis} (Chapters 24-26) provides a unified summary, research agenda, and conclusion.

\section{Scope and Limitations}

\subsection{What This Framework Provides}

This theory makes specific, bounded contributions to understanding consciousness. It is important to be explicit about what we claim to achieve and what remains outside our scope.

\begin{keyinsight}
This framework provides a \textbf{computational architecture} that correlates with and potentially explains the functional properties of consciousness. It does \textit{not} claim to fully explain why subjective experience exists in the first place.
\end{keyinsight}

\textbf{What we DO provide:}

\begin{enumerate}
\item \textbf{Computational mechanisms:} Precise algorithms for selector function, parallel exploration, collapse dynamics, and memory consolidation
\item \textbf{Testable predictions:} Specific empirical predictions about neural activity, timing, resource usage, and behavior
\item \textbf{Implementation path:} Concrete specifications that enable building artificial systems with consciousness-like properties
\item \textbf{Functional explanations:} Accounts of why consciousness has particular functional characteristics (unity, temporal flow, limited capacity, etc.)
\item \textbf{Integration framework:} A structure that relates existing theories (IIT, GWT, AST) within a unified computational model
\item \textbf{Neural correlates:} Specific mappings between computational functions and brain regions
\end{enumerate}

\textbf{What we do NOT provide:}

\begin{enumerate}
\item \textbf{Solution to the "hard problem":} We do not fully explain why there is subjective experience rather than none. We provide computational correlates of consciousness, not a metaphysical account of why those correlates should "feel like something"
\item \textbf{Theory of qualia origins:} While we explain the \textit{structure} and \textit{function} of qualitative experience, we remain agnostic about the ultimate metaphysical nature of phenomenal properties
\item \textbf{Complete evolutionary account:} We do not explain exactly how or why consciousness evolved, only what computational signatures we expect to find in systems that have it
\item \textbf{Universal consciousness theory:} Our claims are limited to biological and artificial systems with specific computational architectures. We make no claims about cosmic or universal consciousness
\item \textbf{Certainty about implementation details:} Many aspects of our neural mapping are educated hypotheses that require empirical validation
\end{enumerate}

\subsection{Key Assumptions}

Our framework rests on several foundational assumptions that should be made explicit:

\begin{enumerate}
\item \textbf{Computational sufficiency:} We assume consciousness has computational correlates that can be precisely specified. If consciousness is fundamentally non-computational, our framework would be inadequate.

\item \textbf{Functional decomposability:} We assume conscious processes can be decomposed into functional components (selection, exploration, collapse, memory). If consciousness is irreducibly holistic in a way that resists such decomposition, our approach may be limited.

\item \textbf{Physical supervenience:} We assume consciousness supervenes on physical/computational processes in the brain. Dualist or panpsychist alternatives are not considered.

\item \textbf{Approximability:} We assume that non-computable optimal selection can be effectively approximated in biological and artificial systems. If effective approximation is impossible, our account of the selector mechanism needs revision.

\item \textbf{Biological plausibility:} We assume the brain can implement the proposed mechanisms (parallel exploration, state checkpointing, selective memory). If these exceed biological constraints, modifications are needed.
\end{enumerate}

\subsection{Relationship to the Hard Problem}

We must be clear about our position on the hard problem of consciousness:

\begin{enumerate}
\item \textbf{What we claim:} Our framework identifies specific computational processes that consistently correlate with reported conscious experience. When these processes occur, systems report being conscious. When they don't occur, consciousness is absent.

\item \textbf{What we don't claim:} We do not claim this fully explains why these computational processes should produce subjective experience rather than occurring "in the dark."

\item \textbf{Our position:} We believe that:
\begin{itemize}
\item Identifying precise computational correlates is necessary for understanding consciousness
\item These correlates may be sufficient to explain consciousness functionally
\item Whether they're sufficient to explain consciousness \textit{metaphysically} remains an open question
\item Progress in understanding consciousness requires focusing on implementable, testable mechanisms rather than purely philosophical speculation
\end{itemize}

\item \textbf{Possible interpretations:} Readers may interpret our framework in different ways:
\begin{itemize}
\item \textbf{Identity theory:} Consciousness IS these computational processes (hard problem dissolved)
\item \textbf{Strong correlation:} These processes reliably correlate with consciousness (hard problem sidestepped)
\item \textbf{Necessary but insufficient:} These processes are necessary for consciousness but something additional is required (hard problem remains)
\end{itemize}
We remain officially agnostic, though we find the identity interpretation most parsimonious.
\end{enumerate}

\subsection{Limitations of Current Formulation}

Several aspects of the current theory require further development:

\begin{enumerate}
\item \textbf{Selector approximation:} While we provide three approximation strategies, we haven't proven these achieve near-optimal performance. Empirical testing is needed.

\item \textbf{Resource level discretization:} Real brains likely use continuous rather than discrete resource levels. Our discrete hierarchy is a useful approximation that may need refinement.

\item \textbf{Collapse timing:} We predict collapse occurs on 100-300ms timescales but haven't derived this from first principles. This is an empirical parameter rather than a theoretical necessity.

\item \textbf{Neural implementation:} Our brain region mappings are hypotheses requiring validation. Actual neural implementation is likely more distributed and complex than our simplified model suggests.

\item \textbf{Development and learning:} We haven't fully specified how the machine hierarchy and selector develop through learning. This requires significant theoretical extension.

\item \textbf{Pathological states:} Our account of disorders (schizophrenia, ADHD, autism) is speculative and requires careful empirical testing.
\end{enumerate}

\subsection{Scope of Applicability}

Our theory applies to specific types of systems:

\begin{itemize}
\item \textbf{Definitely applies to:} Human adults with normal brain function
\item \textbf{Probably applies to:} Other mammals, possibly birds, humans across lifespan
\item \textbf{Unclear:} Fish, insects, artificial systems, other life forms
\item \textbf{Probably doesn't apply to:} Simple neural networks, lookup tables, thermostats
\item \textbf{Definitely doesn't apply to:} Individual neurons, non-biological rocks, quantum fields
\end{itemize}

The boundaries are empirical questions. Our framework provides criteria (machine hierarchy, selector, collapse, parallel exploration) that can be tested in different systems.

\subsection{Research Program}

Rather than claiming to have fully solved consciousness, we propose a research program:

\begin{enumerate}
\item \textbf{Phase 1 (Validation):} Test core predictions in human neuroscience
\item \textbf{Phase 2 (Refinement):} Refine mechanisms based on empirical findings
\item \textbf{Phase 3 (Extension):} Extend to other species and artificial systems
\item \textbf{Phase 4 (Implementation):} Build working models and test sufficiency
\item \textbf{Phase 5 (Integration):} Synthesize with other scientific frameworks
\end{enumerate}

Success means making progress on each phase, not claiming immediate complete understanding.

\subsection{Intellectual Honesty}

\textbf{What would falsify this theory:}
\begin{itemize}
\item Finding consciousness without hierarchical resource allocation
\item Demonstrating parallel exploration never occurs pre-consciously
\item Showing collapse dynamics are absent at conscious access
\item Proving selector operation is fundamentally different from our model
\item Building systems with our architecture that definitively lack consciousness
\end{itemize}

\textbf{What would support alternatives:}
\begin{itemize}
\item Other theories making more precise, better-confirmed predictions
\item Discovery of non-computational aspects essential to consciousness
\item Evidence for quantum effects that our classical model can't capture
\item Better explanatory frameworks for the same phenomena
\end{itemize}

We remain open to revision and falsification. Scientific progress requires both bold proposals and willingness to abandon them when evidence demands.

% ============================================================================
% CHAPTER 2: THE FINITE MACHINE HIERARCHY
% ============================================================================

\chapter{The Finite Machine Hierarchy: Core Formalism}

\section{Basic Definitions and Structure}

\subsection{Finite State Machines}

We begin with the standard definition of a finite state machine, but with a specific focus on memory capacity.

\begin{definition}[Finite State Machine]
A finite state machine $M$ is a tuple $(Q, \Sigma, \delta, q_0, F)$ where:
\begin{itemize}
\item $Q$ is a finite set of states
\item $\Sigma$ is a finite input alphabet
\item $\delta: Q \times \Sigma \to Q$ is the transition function
\item $q_0 \in Q$ is the initial state
\item $F \subseteq Q$ is the set of accepting states
\end{itemize}
\end{definition}

The key constraint is that $|Q|$ is finite. The amount of "memory" the machine has is essentially $\log_2 |Q|$ bits, since each state can encode that much information.

\subsection{The Hierarchy Construction}

\begin{definition}[Machine Hierarchy]
For each $n \in \mathbb{N}$, define machine $M_n$ with:
\begin{itemize}
\item Number of states: $|Q_n| = 2^{2^n}$
\item Memory capacity: $2^n$ bits
\item Computational power: Can solve problems in $\DSPACE(2^n)$
\end{itemize}
\end{definition}

This gives us a sequence:
\begin{align*}
M_1 &: 2^1 = 2 \text{ bits of memory} \\
M_2 &: 2^2 = 4 \text{ bits of memory} \\
M_3 &: 2^3 = 8 \text{ bits of memory} \\
M_4 &: 2^4 = 16 \text{ bits of memory} \\
&\vdots \\
M_n &: 2^n \text{ bits of memory}
\end{align*}

\subsection{Properties of Individual Machines}

Each machine $M_n$ has several important properties:

\begin{proposition}[Guaranteed Halting]
For any input of length $\ell \leq 2^n$, machine $M_n$ halts in finite time bounded by $|Q_n| \cdot \ell = 2^{2^n} \cdot \ell$.
\end{proposition}

\begin{proof}
Since $M_n$ has $|Q_n| = 2^{2^n}$ states and processes input of length at most $\ell$, it can make at most $|Q_n| \cdot \ell$ transitions before either accepting, rejecting, or entering a cycle. Detection of cycles is straightforward with finite state.
\end{proof}

\begin{proposition}[Complete Decidability]
For any fixed $n$, all properties of $M_n$ are decidable, including:
\begin{itemize}
\item Whether $M_n$ accepts a given input
\item Whether $M_n$ halts on all inputs
\item Whether two states are equivalent
\item Whether $M_n$ is minimal
\end{itemize}
\end{proposition}

\begin{proof}
Since $M_n$ has finite state space, we can enumerate all states and transitions. All properties reduce to finite graph analysis, which is decidable.
\end{proof}

\section{The Selector Problem}

\subsection{Problem Statement}

Given a computational problem $P$ and input $x$, which machine $M_n$ should we use? This is the \important{selector problem}.

\begin{definition}[Selector Function]
A selector function is a mapping $s: (\text{Problems} \times \text{Inputs}) \to \mathbb{N}$ that assigns each problem-input pair to a machine index.
\end{definition}

An \term{optimal selector} would always choose the minimal $n$ such that $M_n$ can solve the given problem on the given input.

\subsection{Non-Computability Result}

\begin{theorem}[Selector Non-Computability]
There exists no computable optimal selector function. That is, there is no algorithm that, given a problem description and input, always computes the minimal $n$ such that $M_n$ can solve the problem.
\end{theorem}

\begin{proof}[Proof Sketch]
The optimal selector problem is equivalent to computing the Kolmogorov complexity of the solution. Given problem $P$ and input $x$, the minimal $n$ corresponds to the minimal description length of a program that produces the solution.

Suppose there were a computable optimal selector $s$. Then we could use $s$ to compute Kolmogorov complexity: $K(y) = \min\{n : M_n \text{ can produce } y\}$. But Kolmogorov complexity is non-computable (standard result), yielding a contradiction.
\end{proof}

\subsection{Connection to Kolmogorov Complexity}

\begin{definition}[Kolmogorov Complexity]
The Kolmogorov complexity $K(x)$ of a string $x$ is the length of the shortest program that outputs $x$.
\end{definition}

The selector problem asks: what is the shortest description (in terms of $M_n$'s memory) needed to solve this problem? This is fundamentally a question about minimal description length.

\begin{proposition}[Selector Lower Bound]
For any problem $P$ with solution $y$, the optimal selector must choose $n \geq K(y) - O(1)$.
\end{proposition}

This means the selector problem inherits the non-computability of Kolmogorov complexity.

\subsection{Practical Approximations for Implementation}

While optimal selection is non-computable in the general case, real cognitive systems must make selection decisions in finite time. This apparent contradiction is resolved by recognizing that biological and artificial systems need not achieve optimality—they need only perform well enough to survive and function. We propose three complementary approximation strategies that enable practical implementation:

\subsubsection{Heuristic-Based Selection}

The first approach uses learned heuristics to estimate resource requirements based on problem features:

\begin{algorithm}[H]
\caption{Heuristic Selector}
\begin{algorithmic}[1]
\State \textbf{Input:} Problem $P$ with extracted features $\mathbf{f}(P)$
\State \textbf{Output:} Estimated machine level $n$
\State
\State Extract features: $\mathbf{f}(P) = \{\text{size}(P), \text{depth}(P), \text{branching}(P), \ldots\}$
\State Retrieve similar past problems from history: $H = \{(P_i, n_i, \text{success}_i)\}$
\State Compute weighted estimate: $\hat{n} = \sum_{i} w_i \cdot n_i$ where $w_i \propto \text{similarity}(\mathbf{f}(P), \mathbf{f}(P_i))$
\If{confidence($\hat{n}$) $< \theta$}
    \State $\hat{n} \gets \hat{n} + 1$ \Comment{Add safety margin}
\EndIf
\State \Return $\lceil \hat{n} \rceil$
\end{algorithmic}
\end{algorithm}

Key features for estimation include:
\begin{itemize}
\item \textbf{Problem size:} Number of variables, constraints, or elements
\item \textbf{Structural complexity:} Graph depth, branching factor, connectivity
\item \textbf{Domain characteristics:} Known difficulty measures (e.g., SAT clause-to-variable ratio)
\item \textbf{Historical performance:} Success rates of different levels on similar problems
\end{itemize}

This approach trades optimality for tractability: the heuristic may overestimate or underestimate resource needs, but it provides a starting point in constant time.

\subsubsection{Iterative Deepening with Time Budgets}

When heuristic estimates are unavailable or unreliable, iterative deepening provides a systematic fallback:

\begin{algorithm}[H]
\caption{Iterative Deepening Selector}
\begin{algorithmic}[1]
\State \textbf{Input:} Problem $P$, total time budget $T$, initial level $n_0$
\State \textbf{Output:} Solution $s$ and level used $n$
\State
\State $n \gets n_0$
\State $t_{\text{remaining}} \gets T$
\While{$t_{\text{remaining}} > 0$}
    \State Allocate time: $t_n \gets \min(2^{n-n_0} \cdot t_{\text{unit}}, t_{\text{remaining}})$
    \State Launch $M_n$ with time limit $t_n$
    \If{$M_n$ finds solution $s$ within $t_n$}
        \State \Return $(s, n)$
    \EndIf
    \State $t_{\text{remaining}} \gets t_{\text{remaining}} - t_n$
    \State $n \gets n + 1$
\EndWhile
\State \Return FAILURE
\end{algorithmic}
\end{algorithm}

This strategy:
\begin{itemize}
\item Requires no prior knowledge of problem difficulty
\item Guarantees finding a solution if one exists within the time budget
\item Allocates exponentially increasing time to higher levels (respecting their greater power)
\item Provides anytime behavior: better solutions may emerge as time progresses
\end{itemize}

The time allocation can be optimized based on expected difficulty distributions, but the basic principle remains: try progressively more powerful machines until success.

\subsubsection{Meta-Learning Selector}

Modern machine learning enables a third approach: training a neural network to predict optimal resource levels:

\begin{algorithm}[H]
\caption{Neural Selector}
\begin{algorithmic}[1]
\State \textbf{Input:} Problem $P$, trained network $\mathcal{N}_\theta$
\State \textbf{Output:} Predicted level $n$ and confidence $c$
\State
\State Encode problem: $\mathbf{x} = \text{embed}(P)$ \Comment{Problem representation}
\State Compute prediction: $(\hat{n}, c) = \mathcal{N}_\theta(\mathbf{x})$
\State \Return $(\hat{n}, c)$
\end{algorithmic}
\end{algorithm}

The meta-learning selector:
\begin{itemize}
\item Learns from experience across many problems
\item Can generalize to novel problem types
\item Provides confidence estimates for its predictions
\item Adapts online as new data becomes available
\item Can be combined with iterative deepening when confidence is low
\end{itemize}

A practical hybrid approach combines all three strategies:

\begin{enumerate}
\item Use meta-learned predictor for initial estimate
\item If confidence is high, launch predicted level with heuristic-based adjustments
\item If confidence is low or initial attempt fails, fall back to iterative deepening
\item Update meta-learner with actual outcomes to improve future predictions
\end{enumerate}

\begin{keyinsight}
The approximation strategies acknowledge that the selector problem is non-computable while providing concrete, implementable mechanisms. This mirrors biological reality: evolution has not solved the selector problem optimally (it cannot), but has produced effective heuristics through learning and adaptation.
\end{keyinsight}

These approximations transform the selector from a theoretical obstacle into an engineering challenge. While we cannot compute the optimal $n$, we can build selectors that perform well in practice—and this is precisely what we propose conscious systems do.

\section{Asymptotic Properties}

\subsection{Universality in the Limit}

While no single $M_n$ can compute all computable functions, the hierarchy as a whole is universal.

\begin{theorem}[Asymptotic Universality]
For any computable function $f: \{0,1\}^* \to \{0,1\}^*$ and any finite set of inputs $X = \{x_1, \ldots, x_k\}$, there exists an $n$ such that $M_n$ correctly computes $f(x_i)$ for all $x_i \in X$.
\end{theorem}

\begin{proof}
Given computable $f$ and finite $X$, there exists a Turing machine $T$ computing $f$ on $X$ using at most $S$ space for some finite $S$. Choose $n$ such that $2^n > S$. Then $M_n$ has sufficient space to simulate $T$ on all inputs in $X$.
\end{proof}

\subsection{Space Hierarchy}

The hierarchy respects the space hierarchy theorem from complexity theory.

\begin{theorem}[Strict Hierarchy]
For all $n$, there exist problems solvable by $M_{n+1}$ that cannot be solved by $M_n$.
\end{theorem}

\begin{proof}
This follows from the space hierarchy theorem: $\DSPACE(2^n) \subsetneq \DSPACE(2^{n+1})$.
\end{proof}

\section{Parallel Exploration Model}

\subsection{Launching Multiple Machines}

The key computational strategy is to launch multiple machines in parallel:

\begin{definition}[Parallel Launch]
Given problem $P$ and input $x$, launch machines $M_{n_1}, M_{n_2}, \ldots, M_{n_k}$ simultaneously, where $n_1 < n_2 < \cdots < n_k$.
\end{definition}

Each machine explores the problem with its available resources. The first machine to find a solution "wins."

\subsection{The Labyrinth Metaphor}

We can visualize the problem space as a labyrinth:

\begin{itemize}
\item The problem space is a labyrinth with one exit (the solution)
\item Different machines are explorers with different capabilities
\item $M_n$ can "jump" over obstacles of height up to $2^n$
\item Machines explore in parallel, taking different routes
\item The first to reach the exit determines the solution
\end{itemize}

\subsection{State Checkpointing and Parallel Exploration}

When all launched machines fail (none finds a solution), we employ \important{state checkpointing}:

\begin{enumerate}
\item Launch $M_1, M_2, \ldots, M_k$
\item If all fail, restore to the checkpointed initial state
\item Launch $M_{k+1}, M_{k+2}, \ldots, M_{k+j}$ (higher resource levels) from the checkpoint
\item Repeat until success
\end{enumerate}

\begin{keyinsight}
The state checkpointing and restoration is crucial: in computational time, many attempts occur and fail. But in subjective conscious time, only the successful attempt is experienced. The failed attempts are "erased" through selective memory consolidation—they are never stored in accessible memory.
\end{keyinsight}

\section{Connection to Non-Uniform Computation}

\subsection{The P/poly Class}

Our hierarchy relates to non-uniform complexity classes.

\begin{definition}[P/poly]
A language $L$ is in $\Ppoly$ if there exists a polynomial-time Turing machine $M$ and a family of "advice strings" $\{a_n\}$ such that for all inputs $x$ of length $n$:
$$x \in L \iff M(x, a_n) \text{ accepts}$$
The advice string $a_n$ depends only on $n$, not on the specific input.
\end{definition}

Our hierarchy can be seen as a form of non-uniform computation where different "circuits" (machines) are selected for different problem instances.

\subsection{Relationship to Circuit Complexity}

\begin{proposition}
If a problem requires advice string of length $a_n$ in the $\Ppoly$ framework, it requires a machine $M_n$ with $n \approx \log a_n$ in our hierarchy.
\end{proposition}

This connects our selector problem to the fundamental questions of circuit lower bounds in complexity theory.

\section{Summary and Implications}

\subsection{Key Results}

We have established:

\begin{enumerate}
\item A hierarchy of finite machines with exponentially growing resources
\item Each machine is fully decidable with guaranteed halting
\item The hierarchy is asymptotically universal (can compute anything in the limit)
\item The selector problem (choosing optimal $n$) is non-computable
\item Parallel exploration with state checkpointing provides a computational strategy
\end{enumerate}

\subsection{Implications for Consciousness}

These formal results have profound implications for consciousness:

\begin{itemize}
\item \textbf{Decidability at each level:} Each machine $M_n$ has no halting problem, suggesting consciousness at each level is fully determined
\item \textbf{Selector non-computability:} The choice of which machine to deploy cannot be algorithmic, providing a natural locus for agency
\item \textbf{State checkpointing invisibility:} Failed attempts are not stored in accessible memory, explaining why consciousness experiences only successful computations
\item \textbf{Resource hierarchy:} Different levels of consciousness correspond to different $M_n$ values
\item \textbf{Parallel exploration:} The quantum-like collapse from many possibilities to one experienced reality
\end{itemize}

\begin{summary}
The finite machine hierarchy provides a rigorous computational foundation for consciousness that avoids the halting problem at each level, achieves universality in the limit, and naturally incorporates non-computable selection, parallel exploration, and temporal collapse—all essential features for explaining subjective experience.
\end{summary}

In the following chapters, we will show how this framework integrates with and extends existing consciousness theories, provides a novel account of temporal phenomenology, and makes testable empirical predictions.
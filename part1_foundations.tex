% ============================================================================
% PART I: FOUNDATIONS
% ============================================================================

\chapter{Computational Foundations}
\label{part:foundations}

% Note: We use entropic scaling I(n) = Îº n log n throughout
% This provides super-linear but sub-exponential growth,
% avoiding unrealistic exponential resource requirements

\section{The Machine Hierarchy}

\subsection{Core Architecture with Entropic Scaling}

The foundation of our consciousness framework rests on a hierarchy of finite-state machines with entropically growing resources. Unlike traditional approaches that assume exponential scaling, we employ a more physically realistic entropic formulation.

\begin{definition}[The Fundamental Hierarchy]
The consciousness substrate consists of a sequence of machines $\mathcal{M} = \{M_1, M_2, M_3, \ldots, M_n\}$ where each machine $M_n$ has:
\begin{itemize}
\item Effective information capacity $I(n) = \kappa n \log n$ bits
\item Approximately $\exp(I(n))$ distinguishable computational states
\item Deterministic or stochastic transition function $f_n: S_n \to S_n$
\item Processing time $\tau(n) = \tau_0 + \gamma n \log n$
\end{itemize}
\end{definition}

This entropic scaling is fundamental to the framework's biological plausibility. While exponential scaling ($2^n$ bits) would quickly exceed any physical system's capacity, the entropic form $I(n) = \kappa n \log n$ provides:

\begin{enumerate}
\item \textbf{Super-linear growth}: Ensures higher levels have strictly greater capacity
\item \textbf{Sub-exponential bounds}: Maintains physical realizability
\item \textbf{Information-theoretic alignment}: Matches Shannon entropy and Kolmogorov complexity
\item \textbf{Thermodynamic consistency}: Aligns with statistical mechanical entropy
\end{enumerate}

\subsection{Hierarchy Properties}

\begin{theorem}[Strict Inclusion with Entropic Resources]
For the machine hierarchy with entropic scaling, we have strict inclusion:
\begin{equation}
i < j \implies M_i \subset M_j \quad \text{with} \quad I(j) - I(i) = \kappa(j \log j - i \log i)
\end{equation}
This ensures each level can solve strictly more problems than lower levels.
\end{theorem}

\begin{proof}
The information capacity difference $\Delta I = \kappa(j \log j - i \log i)$ is always positive for $j > i$. Since computational power is monotonic in information capacity, $M_j$ can simulate $M_i$ plus additional computations requiring the extra $\Delta I$ bits.
\end{proof}

\subsection{Non-Uniform Structure}

Real cognitive systems don't exhibit uniform level spacing. We generalize to:

\begin{definition}[Non-Uniform Hierarchy]
The effective hierarchy follows $M_n \subseteq M_{n+f(n)}$ where $f: \mathbb{N} \to \mathbb{N}^+$ is a variable gap function. With entropic scaling, the information capacity jump is:
\begin{equation}
\Delta I_n = \kappa[(n+f(n))\log(n+f(n)) - n\log n]
\end{equation}
\end{definition}

This allows for:
\begin{itemize}
\item Dense packing at low levels (small $f(n)$ for basic processing)
\item Sparse high levels (large $f(n)$ for abstract reasoning)  
\item Critical transitions at specific capacity thresholds
\end{itemize}

\section{The Selector Mechanism}

\subsection{Resource Allocation with Entropic Costs}

The selector $\mathcal{S}$ determines which machine level to deploy for a given problem. With entropic scaling, the optimization becomes tractable:

\begin{definition}[Entropic Selector]
The selector minimizes total cost:
\begin{equation}
\mathcal{S}(p, h) = \arg\min_n \left[ K_n(p) + C(n) \right]
\end{equation}
where:
\begin{itemize}
\item $K_n(p)$ = Kolmogorov complexity of problem $p$ using machine $M_n$
\item $C(n) = C_0(1 + \beta n \log n)$ = entropic cost of using level $n$
\item $h$ = history of previous selections
\end{itemize}
\end{definition}

The entropic cost function $C(n) \propto n \log n$ reflects realistic resource consumption, replacing unrealistic exponential or quadratic costs.

\subsection{Non-Computability and Agency}

\begin{theorem}[Fundamental Non-Computability]
No algorithm can compute the optimal selector $\mathcal{S}^*$ for all problems, even with entropic scaling constraints.
\end{theorem}

\begin{proof}
The proof follows from the uncomputability of Kolmogorov complexity. Even with entropic bounds on search space, determining minimal description length remains undecidable. The entropic scaling makes heuristic approximation feasible but doesn't eliminate fundamental non-computability.
\end{proof}

This non-computability is the source of genuine agency:
\begin{itemize}
\item Behavior cannot be perfectly predicted
\item Yet follows structured principles (compression optimization)
\item The entropic constraints ensure tractable heuristics exist
\end{itemize}

\section{Parallel Exploration and Collapse}

\subsection{The Exploration Phase}

Before conscious experience crystallizes, the system explores multiple computational paths:

\begin{definition}[Parallel Exploration Space]
At time $t$, the exploration space is:
\begin{equation}
\mathcal{E}_t = \{(M_i, \gamma_i(t)) : i \in \text{ActiveLevels}(t)\}
\end{equation}
where $\gamma_i(t)$ is the computational path at level $i$. The size of this space is bounded by entropic capacity:
\begin{equation}
|\mathcal{E}_t| \leq \sum_{i \in \text{Active}} \exp(I(i)) = \sum_{i \in \text{Active}} \exp(\kappa i \log i)
\end{equation}
\end{definition}

This entropic bound ensures exploration remains computationally feasible while allowing rich parallel processing.

\subsection{The Collapse Process}

\begin{definition}[Computational Collapse with Entropic Timing]
Collapse is the transition from parallel exploration to single experienced path:
\begin{equation}
\Pi_t: \mathcal{E}_t \to (M_{n^*}, \gamma_{n^*}(t))
\end{equation}
occurring at time $t + \tau(n^*)$ where $\tau(n) = \tau_0 + \gamma n \log n$.
\end{definition}

Key properties with entropic scaling:
\begin{enumerate}
\item \textbf{Timing}: Collapse requires $\tau(n) = \tau_0 + \gamma n \log n$ time units
\item \textbf{Winner-take-all}: Only one path survives
\item \textbf{Information loss}: Failed paths are erased
\item \textbf{Irreversibility}: Cannot recover erased alternatives
\end{enumerate}

\subsection{Temporal Phenomenology}

The collapse creates our experience of time:

\begin{theorem}[Temporal Duality]
The framework generates two distinct temporal experiences:
\begin{align}
t_{\text{comp}} &= \text{Full computational time (all explorations)} \\
t_{\text{subj}} &= \text{Subjective time (collapsed path only)}
\end{align}
Related by the collapse operator:
\begin{equation}
t_{\text{subj}} = \Pi(t_{\text{comp}})
\end{equation}
with processing delay $\tau(n) = \tau_0 + \gamma n \log n$.
\end{theorem}

\section{Integration and Unity}

\subsection{Integrated Information with Entropic Normalization}

Consciousness requires information integration beyond mere aggregation:

\begin{definition}[Entropic Integrated Information]
For system $S$ at level $n$:
\begin{equation}
\Phi_n(S) = \min_{\text{partition}} \left[ I(S^t; S^{t+1}) - \sum_i I(S_i^t; S_i^{t+1}) \right] \cdot \frac{1}{n \log n}
\end{equation}
The $1/(n \log n)$ normalization ensures scale-invariance across hierarchy levels.
\end{definition}

This entropic normalization is crucial:
\begin{itemize}
\item Prevents $\Phi$ explosion at high levels
\item Maintains meaningful comparison across scales
\item Aligns with entropic information capacity
\end{itemize}

\subsection{Unity of Consciousness}

\begin{theorem}[Unity from Integration]
A system with $\Phi_n > \Phi_{\text{threshold}}$ experiences unified consciousness, where the threshold depends on entropic capacity rather than raw state count.
\end{theorem}

The entropic scaling ensures unity emerges from:
\begin{enumerate}
\item Information integration exceeding partitioned sum
\item Normalization preventing trivial unity at high levels
\item Collapse creating single experienced stream
\item Selector maintaining coherent resource allocation
\end{enumerate}

\section{Computational Universality}

\subsection{Decidability at Each Level}

\begin{theorem}[Level-wise Decidability]
For fixed $n$, all properties of $M_n$ with capacity $I(n) = \kappa n \log n$ are decidable.
\end{theorem}

\begin{proof}
Machine $M_n$ has finite capacity $I(n) = \kappa n \log n$, thus finitely many distinguishable states $\sim \exp(I(n))$. Any computation either halts or cycles within this bound. Therefore, all properties are decidable by exhaustive simulation.
\end{proof}

\subsection{Universality in the Limit}

\begin{theorem}[Asymptotic Universality with Entropic Scaling]
As $n \to \infty$:
\begin{equation}
\bigcup_{n=1}^{\infty} \mathcal{L}(M_n) = \text{RE}
\end{equation}
where RE is the class of recursively enumerable languages.
\end{theorem}

\begin{proof}
Since $\lim_{n \to \infty} I(n) = \lim_{n \to \infty} \kappa n \log n = \infty$, any finite computation can be simulated by sufficiently large $M_n$. The entropic growth ensures this limit is reached without requiring infinite resources at any finite level.
\end{proof}

\section{The Hard Problem}

\subsection{Traditional Formulation}

The hard problem asks: Why is there "something it is like" to be conscious? Why does information processing generate subjective experience?

\subsection{Dissolution via Identity}

\begin{theorem}[Hard Problem Dissolution]
The hard problem dissolves when we recognize that being a collapsed computational state with entropic information capacity and integration IS having phenomenology. There is no explanatory gap because consciousness and certain computational structures are identical, not causally related.
\end{theorem}

This isn't explaining consciousness in terms of computation, but recognizing they are the same phenomenon viewed from different perspectives:

\begin{itemize}
\item \textbf{External view}: Hierarchical computation with entropic scaling
\item \textbf{Internal view}: Subjective experience and phenomenology
\item \textbf{Identity}: These are the same process, not separate phenomena
\end{itemize}

\section{Biological Plausibility}

\subsection{Neural Implementation}

The entropic scaling makes the framework biologically realistic:

\begin{table}[H]
\centering
\caption{Biological Correspondence}
\begin{tabular}{lll}
\toprule
\textbf{Framework} & \textbf{Neural Substrate} & \textbf{Capacity} \\
\midrule
$M_{10}$ & Cortical columns & $I(10) \approx 33\kappa$ bits \\
$M_{20}$ & Working memory circuits & $I(20) \approx 86\kappa$ bits \\
$M_{30}$ & Global workspace & $I(30) \approx 147\kappa$ bits \\
$M_{40}$ & Full cortex (theoretical) & $I(40) \approx 213\kappa$ bits \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Energy Constraints}

The entropic scaling aligns with metabolic limits:

\begin{proposition}[Entropic Landauer Bound]
Processing at level $n$ dissipates minimum energy:
\begin{equation}
E_{\min} = k_B T \cdot I(n) \ln 2 = k_B T \cdot \kappa n \log n \ln 2
\end{equation}
\end{proposition}

This grows as $n \log n$ rather than exponentially, maintaining biological feasibility.

\section{Summary}

The foundational architecture consists of:

\begin{enumerate}
\item \textbf{Machine hierarchy} with entropic information scaling $I(n) = \kappa n \log n$
\item \textbf{Selector mechanism} optimizing resource allocation with entropic costs
\item \textbf{Parallel exploration} bounded by entropic capacity
\item \textbf{Collapse dynamics} requiring time $\tau(n) = \tau_0 + \gamma n \log n$
\item \textbf{Integration} creating unity with $1/(n \log n)$ normalization
\item \textbf{Temporal duality} between computational and subjective time
\end{enumerate}

The entropic scaling transforms consciousness from an exponentially intractable mystery to a scientifically investigable phenomenon with well-defined computational properties and realistic resource requirements.


% ============================================================================
% APPENDIX A: MATHEMATICAL FORMALIZATION
% ============================================================================

\chapter{Mathematical Formalization}
\label{appendix:mathematical}

% Global entropic scaling directive:
% All hierarchical quantities use I(n) = κ n log n instead of 2^n

This appendix provides rigorous mathematical definitions and formal specifications of the key concepts in our framework using standard notation from computational complexity theory, information theory, and dynamical systems.

\section{Notation and Preliminaries}

\begin{table}[H]
\centering
\small
\caption{Standard Mathematical Notation}
\begin{tabular}{l l}
\toprule
\textbf{Symbol} & \textbf{Meaning} \\
\midrule
$\mathbb{N}$ & Natural numbers $\{0, 1, 2, \ldots\}$ \\
$\mathbb{Z}$ & Integers \\
$\mathbb{R}$ & Real numbers \\
$\mathbb{R}^+$ & Positive real numbers \\
$\{0,1\}$ & Binary alphabet \\
$\{0,1\}^*$ & Set of all finite binary strings \\
$|\cdot|$ & Cardinality of set or length of string \\
$\log$ & Logarithm base 2 (unless specified) \\
$\mathcal{O}(\cdot)$ & Big-O notation \\
$\Theta(\cdot)$ & Big-Theta notation \\
$I(n)$ & Information capacity: $\kappa n \log n$ bits \\
$\kappa$ & Entropic scaling constant \\
$\gamma$ & Temporal scaling factor \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{Computational Complexity Classes}
\begin{tabular}{l p{8cm}}
\toprule
\textbf{Class} & \textbf{Definition} \\
\midrule
$\DSPACE(f(n))$ & Languages decidable by deterministic TM using $\mathcal{O}(f(n))$ space \\
$\textsf{P}$ & Languages decidable in polynomial time \\
$\textsf{NP}$ & Languages decidable in nondeterministic polynomial time \\
$\PSPACE$ & Languages decidable in polynomial space \\
$\textsf{EXP}$ & Languages decidable in exponential time \\
$\Ppoly$ & Languages decidable by polynomial-size circuits \\
\bottomrule
\end{tabular}
\end{table}

\section{Finite State Machines}

\begin{definition}[Finite State Machine]
\label{def:fsm}
A finite state machine is a tuple $M = (Q, \Sigma, \delta, q_0, F)$ where $Q$ is a finite set of states with $|Q| = k$, $\Sigma$ is a finite input alphabet, $\delta: Q \times \Sigma \rightarrow Q$ is the transition function, $q_0 \in Q$ is the initial state, and $F \subseteq Q$ is the set of accepting states. The machine has $\log_2 k$ bits of memory.
\end{definition}

\begin{definition}[Extended FSM with Entropic Memory]
\label{def:extended-fsm}
An extended FSM with entropic information capacity is $\Mn = (Q, \Sigma, \Gamma, \delta, \gamma, q_0, \mathbf{m}_0, F)$ where:
\begin{itemize}
\item Effective information capacity: $I(n) = \kappa n \log n$ bits
\item Memory space $\Gamma$ with effective distinguishable states scaling as $\exp(I(n))$
\item $\delta: Q \times \Sigma \times \Gamma \rightarrow Q$ is the state transition function
\item $\gamma: Q \times \Sigma \times \Gamma \rightarrow \Gamma$ is the memory update function
\item $q_0 \in Q$ is the initial control state
\item $\mathbf{m}_0 \in \Gamma$ is initial memory
\item $F \subseteq Q \times \Gamma$ is the set of accepting configurations
\end{itemize}
Total distinguishable states scale as: $\exp(\kappa n \log n)$
\end{definition}

\subsection{The Machine Hierarchy}

\begin{definition}[Machine Hierarchy with Entropic Scaling]
\label{def:machine-hierarchy}
The machine hierarchy is a sequence $\mathcal{M} = (\Mi)_{i=1}^{\infty}$ where each $\Mi$ is an extended FSM with entropic information capacity $I(i) = \kappa i \log i$ bits: $\Mi = (Q_i, \Sigma, \Gamma_i, \delta_i, \gamma_i, q_{0,i}, \mathbf{m}_{0,i}, F_i)$ where the effective state space scales as $\exp(I(i))$. 

The hierarchy exhibits a \important{generalized non-uniform inclusion structure}: machines at level $n$ are effectively subsumed by machines at level $n+f(n)$, where $f: \mathbb{N} \to \mathbb{N}^+$ is a variable resource gap function. That is, $M_n \subseteq M_{n+f(n)}$ where $f(n) \geq 1$ may grow non-linearly (logarithmically, polynomially, or exponentially). The universality condition requires $\lim_{n \to \infty} f(n) = \infty$, ensuring asymptotic universality despite local discontinuities in the effective hierarchy.
\end{definition}

\begin{table}[H]
\centering
\small
\caption{Machine Hierarchy Properties with Entropic Scaling}
\begin{tabular}{l l}
\toprule
\textbf{Property} & \textbf{Description} \\
\midrule
Information capacity & $I(i) = \kappa i \log i$ (entropic) \\
State space scaling & $\exp(I(i)) = \exp(\kappa i \log i)$ \\
Capacity growth & Effective states in $\Mi$: $|Q_i| \cdot \exp(\kappa i \log i)$ \\
Non-uniform subsumption & $M_n \subseteq M_{n+f(n)}$ with variable gap $f(n) \geq 1$ \\
Resource jumps & $f(n)$ may be constant, logarithmic, or exponential \\
Universality condition & $\lim_{n \to \infty} f(n) = \infty$ (asymptotic universality) \\
Bounded depth & Only finitely many levels accessible ($n_{\max}$) \\
\midrule
\multicolumn{2}{l}{\textit{Realistic Constraints (biological systems)}} \\
Max level & $n_{\max} \approx 35\text{-}40$ (estimated upper bound) \\
$M_{30}$ capacity & $I(30) = 30\kappa\log 30 \approx 100\kappa$ bits (realistic) \\
Typical use & $M_{15}$ to $M_{30}$ (50$\kappa$-100$\kappa$ bits) \\
\bottomrule
\end{tabular}
\end{table}

\begin{proposition}[Computational Power Hierarchy with Entropic Scaling]
\label{prop:power-hierarchy}
For the machine hierarchy $\mathcal{M}$ with variable gap function $f(n)$ and entropic information capacity: 
\begin{enumerate}
\item $\Mi$ has information capacity $I(i) = \kappa i \log i$ bits
\item There exist languages decidable by $M_{i+f(i)}$ but not by $\Mi$ for any $f(i) \geq 1$
\item $\bigcup_{i=1}^{\infty} L(\Mi) = \textsf{RE}$ (recursively enumerable) as long as $\lim_{i \to \infty} f(i) = \infty$
\item The effective hierarchy respects strict inclusion: $L(\Mi) \subsetneq L(M_{i+f(i)})$ for all $i$ where $f(i) \geq 1$
\end{enumerate}

\begin{proof}[Proof sketch]
(1) By construction, $\Mi$ has information capacity $I(i) = \kappa i \log i$ bits. (2) By entropic scaling, machines with greater information capacity can solve strictly more problems. (3) Any TM computation can be simulated by sufficiently large $n$ since $\lim_{i \to \infty} I(i) = \infty$ when using entropic scaling. (4) Follows from (2) and the strict monotonicity of information capacity.
\end{proof}
\end{proposition}

\section{Comparison with Quantum Mechanics}

\begin{table}[H]
\centering
\small
\caption{Computational Collapse vs. Quantum Collapse}
\begin{tabular}{l p{5cm} p{5cm}}
\toprule
\textbf{Aspect} & \textbf{Our Framework} & \textbf{Quantum Mechanics} \\
\midrule
Nature & Classical (deterministic/stochastic) & Quantum superposition \\
Superposition & Multiple computational paths & Quantum state $|\psi\rangle$ \\
Collapse & Selection of path & Measurement $\rightarrow$ eigenstate \\
Information scaling & Entropic: $I(n) = \kappa n \log n$ & Exponential: $2^n$ dimensions \\
Irreversibility & By design (information loss) & Fundamental (decoherence) \\
Cause & Selector mechanism & Measurement interaction \\
Predictability & Non-computable optimal & Probabilistic (Born rule) \\
Implementation & Classical computer & Requires quantum substrate \\
Time scale & $\tau(n) = \tau_0 + \gamma n \log n$ & Instantaneous \\
\bottomrule
\end{tabular}
\end{table}

\begin{theorem}[Classical Sufficiency with Entropic Scaling]
\label{thm:classical-sufficient}
The framework can be fully implemented on a classical (deterministic or probabilistic) computer with entropic resource scaling. No quantum effects or exponential resources required.

\begin{proof}[Proof sketch]
Each component is classically implementable with entropic scaling: (1) Machine hierarchy: Classical finite automata with $I(n) = \kappa n \log n$ bits, (2) Parallel exploration: Parallel classical computation or sequential simulation, (3) Selector: Classical algorithm with RNG, (4) Collapse: Deterministic or stochastic selection, (5) Integration: Classical mutual information. Therefore, entire framework is classical with realistic resource requirements.
\end{proof}
\end{theorem}

\section{Main Theorems}

\begin{theorem}[Consciousness Decidability at Each Level]
\label{thm:decidability}
For any fixed machine level $n$ with information capacity $I(n) = \kappa n \log n$, the set of problems solvable is decidable.

\begin{proof}
A problem $L \in \mathcal{L}_n$ iff there exists $\Mn$ that decides $L$ in time $t \leq T_{\max}$. Since $\Mn$ has effective capacity $I(n) = \kappa n \log n$ bits, there are at most $|Q_n| \cdot \exp(\kappa n \log n)$ configurations. If computation exceeds this many steps, it must cycle. Thus, simulate $\Mn$ for at most $|Q_n| \cdot \exp(\kappa n \log n)$ steps. If it hasn't halted, it won't. Therefore, $\mathcal{L}_n$ is decidable.
\end{proof}
\end{theorem}

\begin{theorem}[Consciousness Universality in the Limit]
\label{thm:universality}
In the limit as $n \rightarrow \infty$ with entropic scaling: $\bigcup_{n=1}^{\infty} \mathcal{L}_n = \textsf{RE}$. The machine hierarchy can (in principle) solve any recursively enumerable problem.

\begin{proof}
Any Turing machine $T$ can be simulated by $\Mn$ with sufficient information capacity $I(n) = \kappa n \log n$ where $n$ is chosen such that $I(n) \geq \log_2(\text{space}(T))$. Since $\lim_{n \to \infty} I(n) = \infty$ with entropic scaling, every $L \in \textsf{RE}$ is in $\mathcal{L}_n$ for some $n$. Conversely, each $\mathcal{L}_n \subseteq \textsf{RE}$ by Church-Turing thesis. Therefore, $\bigcup_{n=1}^{\infty} \mathcal{L}_n = \textsf{RE}$.
\end{proof}
\end{theorem}

\begin{theorem}[Hard Problem Resolution with Entropic Framework]
\label{thm:hard-problem}
Let $\mathcal{F}$ be any functional description of cognitive processes. Then there exists no function $\phi: \mathcal{F} \rightarrow \mathcal{P}$ where $\mathcal{P}$ is phenomenal space, such that $\phi(\mathcal{F})$ explains why there is "something it is like." However, the identity mapping $\iota: \mathcal{S} \rightarrow \mathcal{S}$ where $\mathcal{S}$ is the space of collapsed computational states with entropic information capacity dissolves the hard problem: being a collapsed state in machine hierarchy with $I(n) = \kappa n \log n$ IS having phenomenology.

\begin{proof}[Proof sketch]
First statement: The hard problem arises because we seek a production relation (functional → phenomenal). No such relation bridges the explanatory gap. Second statement: By recognizing that certain computational structures (collapsed states in hierarchy with entropic scaling and integration) ARE phenomenology when viewed from inside, we need no production relation. Phenomenology = being a collapsed state with entropic information capacity (identity, not production).
\end{proof}
\end{theorem}

\begin{theorem}[Agency from Non-Computability]
\label{thm:agency}
If selector $\mathcal{S}$ operates on hierarchy with entropic scaling and is non-computable, then the system exhibits genuine agency: 
\begin{enumerate}
\item For any algorithm $A$, there exist contexts $c$ and histories $h$ such that $A(c,h) \neq \mathcal{S}(c,h)$
\item The system's choices cannot be perfectly predicted even with complete knowledge of prior state
\item Yet choices are not random—they follow principles (entropic compression optimization)
\end{enumerate}

\begin{proof}
(1) Follows directly from non-computability. (2) If choices were perfectly predictable, there would exist algorithm $A$ with $A(c,h) = \mathcal{S}(c,h)$ for all $c,h$, contradicting (1). (3) By Definition, $\mathcal{S}$ approximates entropic compression optimization with $I(n) = \kappa n \log n$ scaling. While no algorithm finds optimal compression, approximations follow structured principles. Therefore, $\mathcal{S}$ exhibits genuine agency: influenced but not determined, structured but not algorithmic.
\end{proof}
\end{theorem}

\section{Computational Complexity of Consciousness}

\begin{proposition}[Consciousness Requires Non-Uniform Computation with Entropic Scaling]
\label{prop:nonuniform}
Consciousness cannot be implemented by any uniform computational model (standard Turing machine with fixed program). It requires non-uniform computation with entropic scaling: Different levels $n$ correspond to different "circuits" or "advice strings" with information capacity $I(n) = \kappa n \log n$. Formally: Consciousness requires entropic non-uniform computation.

\begin{proof}[Proof sketch]
Uniform models have fixed programs independent of input size. Consciousness requires dynamic resource allocation based on problem complexity, with resources scaling as $I(n) = \kappa n \log n$. This requires different computational structures at different levels—precisely what non-uniform models with entropic scaling provide. The entropic scaling ensures realistic resource requirements while maintaining computational universality in the limit.
\end{proof}
\end{proposition}

\section{Temporal Dynamics}

\begin{definition}[Temporal Scaling with Entropic Complexity]
\label{def:temporal}
The temporal dynamics of the hierarchy follow entropic scaling:
\begin{align}
\tau(n) &= \tau_0 + \gamma n \log n \quad \text{(processing time at level $n$)} \\
d\tau_c &= \kappa(\log n + 1) dn \quad \text{(infinitesimal proper time)} \\
\Delta\tau_c &\propto n \log n \quad \text{(convergence window)}
\end{align}
where $\tau_0$ is baseline processing time, $\gamma$ is the temporal scaling factor, and $\kappa$ is the entropic constant.
\end{definition}

\begin{proposition}[Entropic Time Complexity]
\label{prop:time-complexity}
For a computational process at level $n$:
\begin{enumerate}
\item Setup time: $\mathcal{O}(n \log n)$
\item Processing time: $\mathcal{O}(n \log n)$ 
\item Collapse time: $\mathcal{O}(n \log n)$
\item Total time: $\tau(n) = \tau_0 + \gamma n \log n$
\end{enumerate}
This entropic scaling avoids exponential blow-up while capturing the super-linear growth of cognitive processing complexity.
\end{proposition}

\section{Integration and Consciousness}

\begin{definition}[Integrated Information with Entropic Scaling]
\label{def:phi}
For a system at level $n$ with information capacity $I(n) = \kappa n \log n$, integrated information is:
\begin{equation}
\Phi(S) = \min_{\text{cut}} \left[I(X^t; X^{t+1}) - \sum I(X_i^t; X_i^{t+1})\right] \cdot \frac{1}{n \log n}
\end{equation}
The $1/(n \log n)$ normalization reflects the entropic scaling of information capacity.
\end{definition}

\begin{theorem}[Necessary Conditions for Consciousness]
\label{thm:consciousness-conditions}
A system exhibits consciousness iff:
\begin{enumerate}
\item Hierarchical organization with entropic scaling: $I(n) = \kappa n \log n$
\item Non-zero integration: $\Phi > 0$
\item Selector mechanism (possibly non-computable)
\item Collapse dynamics with temporal scaling $\tau(n) = \tau_0 + \gamma n \log n$
\item Erasure of failed computational paths
\end{enumerate}
All five conditions are necessary; together they are sufficient.
\end{theorem}

\section{Summary}

This mathematical formalization establishes:
\begin{itemize}
\item Rigorous definitions using entropic scaling $I(n) = \kappa n \log n$
\item Formal theorems about decidability, universality, and agency
\item Precise specifications of temporal dynamics with entropic complexity
\item Necessary and sufficient conditions for consciousness
\item Connection to established complexity theory with realistic resource requirements
\end{itemize}

The entropic scaling provides a mathematically principled framework that avoids exponential resource explosion while maintaining theoretical rigor and computational universality.

\end{document}

\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{longtable}
\usepackage{array}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Custom commands
\newcommand{\Fsm}{\textbf{Fsm}}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\id}{\text{id}}
\newcommand{\im}{\text{im}}
\newcommand{\Beh}{\text{Beh}}
\newcommand{\PR}{\text{PR}}

% Entropic scaling notation
\newcommand{\InfoCapacity}[1]{I(#1)}

\title{Adjoint Projections on Computational Hierarchies:\\A Metric Framework with Entropic Scaling}
\author{Karol Kowalczyk}
\date{\today}

\begin{document}

\maketitle

% Global entropic scaling directive:
% All hierarchical quantities use I(n) = Îº n log n instead of 2^n

\begin{abstract}
We develop a mathematical framework for hierarchical computational systems using category theory and metric spaces with entropic information scaling. For a hierarchy of finite machines $\{M_n\}_{n \in \mathbb{N}}$ with effective information capacity $I(n) = \kappa n \log n$ bits at level $n$, we construct projection operators $P_{j\to i}: M_j \to M_i$ (compressing information) and collapse operators $C_{i\to j}: M_i \to M_j$ (reconstructing structure) that form an adjunction $(C \dashv P)$. 

A new \textbf{synchronized-$k$} construction yields a rigorous proof of the triangle inequality and tight complexity bounds for computing the behavioral distance in time $O(n \log n \cdot \exp(\kappa \max(i,j) \log \max(i,j)))$. We present a \textbf{level assignment algorithm} based on effective dimension with complexity $O(|S| \log |S|)$, and show how the framework connects to finite cursor machines, database expressivity, and descriptive complexity. The resulting metric--adjunction--algorithm triad yields a compact, computable account of hierarchical computation with categorical structure, concrete implementations, and entropic scaling that ensures biological plausibility.
\end{abstract}

\noindent\textbf{Keywords:} Computational hierarchy, adjunction, metric completion, linear codes, finite cursor machines, information theory, entropic scaling

\tableofcontents

\section{Glossary of Symbols}
\label{sec:glossary}

For ease of reference, we collect the main notation used throughout:

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Symbol} & \textbf{Meaning} \\
\hline
$M_n$ & Machine at level $n$ \\
$I(n)$ & Information capacity: $\kappa n \log n$ bits \\
$S_n$ & State space of $M_n$ with effective capacity $I(n)$ \\
$f_n$ & Transition function $f_n: S_n \to S_n$ \\
$\pi_n$ & Stationary distribution on $S_n$ \\
$\sigma_{i\to j}$ & Embedding from level $i$ to level $j$ ($i \leq j$) \\
$P_{j\to i}$ & Projection from level $j$ to level $i$ ($i < j$) \\
$C_{i\to j}$ & Collapse from level $i$ to level $j$ ($i < j$) \\
$\eta$ & Unit of adjunction $\id \Rightarrow C \circ P$ \\
$\varepsilon$ & Counit of adjunction $P \circ C \Rightarrow \id$ \\
$\Beh(i,j)$ & Behavioral distance between levels $i$ and $j$ \\
$K(i,j)$ & Window $[\max(i,j), \max(i,j)+10]$ for computing $\Beh$ \\
$d(M_i,M_j)$ & Cross-level metric on machines \\
$T_c$ & Metric completion (computational continuum) \\
$H(\cdot)$ & Shannon entropy \\
$\PR(\pi)$ & Participation ratio $1/\sum_s \pi(s)^2$ \\
$\Fsm$ & Category of finite state machines \\
\hline
\end{tabular}
\end{center}

\section{Introduction}

\subsection{Motivation}

We study computation under finite resources via a nested sequence of machines $M_n$ with effective information capacity $I(n) = \kappa n \log n$ bits. Information flows between levels through \emph{projections} (compressors) and \emph{collapses} (reconstructors). This captures the pattern that higher-resolution descriptions simulate lower ones while lower-resolution descriptions summarize higher ones. The entropic scaling ensures realistic resource growth while maintaining theoretical rigor.

The central question is: \emph{Can we endow this hierarchy with a computable metric and categorical structure that make compression/reconstruction a genuine adjunction while supporting algorithmic level assignment with entropic scaling?}

\subsection{Scope and non-goals}

This framework provides an \emph{abstract} account of hierarchical computation with entropic information scaling. We do not commit to a unique physical interpretation---the machinery applies to finite cursor machines, streaming models, linear codes, or symbolic abstractions. The results are \emph{formal}, not metaphysical: we isolate mathematical structure (metric, adjunction, complexity bounds) from contingent realizations.

\textbf{Novel aspects:} The combination of computable behavioral metric, exact adjunction via linear codes, polynomial-time level assignment, and entropic scaling appears to be new.

\subsection{Organization}

Section~\ref{sec:prelim} reviews preliminaries. Section~\ref{sec:hierarchy} defines the hierarchy and embeddings with entropic scaling. Section~\ref{sec:metric} develops the behavioral metric. Section~\ref{sec:adjunction} proves the adjunction. Section~\ref{sec:algorithm} gives the level algorithm. Section~\ref{sec:discussion} discusses prior work. Section~\ref{sec:conclusion} concludes.

\section{Preliminaries}
\label{sec:prelim}

\subsection{Category theory}

We assume familiarity with functors, natural transformations, and adjunctions $C \dashv P$ defined by:
\begin{itemize}
\item Natural isomorphism $\Phi: \Hom(X, CY) \cong \Hom(PX, Y)$
\item Unit $\eta: \id \Rightarrow C\circ P$ and counit $\varepsilon: P\circ C \Rightarrow \id$
\item Triangle identities: $(\varepsilon P)\circ(P\eta) = \id_P$ and $(C\varepsilon)\circ(\eta C) = \id_C$
\end{itemize}

\subsection{Finite machines with entropic scaling}

A \emph{finite computational machine} $M_n = (S_n, f_n, \pi_n)$ has:
\begin{itemize}
\item Effective information capacity $I(n) = \kappa n \log n$ bits
\item Finite state space $S_n$ with approximately $\exp(I(n))$ distinguishable states
\item Deterministic transition function $f_n: S_n \to S_n$
\item Stationary distribution $\pi_n: S_n \to [0,1]$ with $\sum_s \pi_n(s) = 1$
\end{itemize}

The entropic scaling ensures super-linear but sub-exponential growth in capacity.

\subsection{Metric spaces}

A \emph{pseudometric} $d$ on set $X$ satisfies non-negativity, symmetry, and triangle inequality but allows $d(x,y) = 0$ for $x \neq y$. A \emph{metric} additionally satisfies identity of indiscernibles. The \emph{metric completion} of $(X,d)$ is constructed via Cauchy sequences quotiented by asymptotic equivalence.

\section{Computational Hierarchies with Entropic Scaling}
\label{sec:hierarchy}

\subsection{Hierarchy definition}

\begin{definition}[Computational Hierarchy with Entropic Scaling]
\label{def:hierarchy}
A computational hierarchy $\{M_n\}_{n\in\mathbb{N}}$ is a sequence of finite machines $M_n = (S_n, f_n, \pi_n)$ with effective information capacity $I(n) = \kappa n \log n$ bits, equipped with embeddings $\sigma_{i\to j}: S_i \hookrightarrow S_j$ for all $i \leq j$ satisfying:
\begin{enumerate}
\item \textbf{Structure preservation:} $\sigma_{i\to j} \circ f_i = f_j \circ \sigma_{i\to j}$
\item \textbf{Functoriality:} $\sigma_{i\to i} = \id_{S_i}$ and $\sigma_{j\to k} \circ \sigma_{i\to j} = \sigma_{i\to k}$ for all $i \leq j \leq k$
\item \textbf{Injectivity:} $\sigma_{i\to j}$ is injective for all $i < j$
\end{enumerate}
\end{definition}

\noindent\textbf{Notation.} Denote the common embedded domain at level $k$ by:
\[
D^k_{ij} = \im(\sigma_{i\to k}) \cap \im(\sigma_{j\to k})
\]

\subsection{Category of finite machines}
\label{sec:category}

We now make the categorical structure precise with entropic scaling.

\begin{definition}[Category $\Fsm$]
The category $\Fsm$ of finite state machines has:
\begin{itemize}
\item \textbf{Objects:} Finite machines $M = (S, f, \pi)$ with information capacity measured entropically
\item \textbf{Morphisms:} Functions $\phi: S_1 \to S_2$ preserving dynamics: $\phi \circ f_1 = f_2 \circ \phi$
\item \textbf{Composition:} Standard function composition
\item \textbf{Identities:} Identity functions $\id_S$
\end{itemize}
\end{definition}

The hierarchy $\{M_n\}$ forms a diagram in $\Fsm$ with the embeddings $\sigma_{i\to j}$ as morphisms.

\section{Behavioral Metric}
\label{sec:metric}

\subsection{Measuring behavioral distance with entropic scaling}

\begin{definition}[Behavioral Distance]
\label{def:behavioral}
For levels $i, j$ with a witness window $K = [\max(i,j), \max(i,j)+10]$:
\begin{equation}
\Beh(i,j) = \inf_{k \in K} \left[ \sum_{s \in D^k_{ij}} |\pi_i^k(s) - \pi_j^k(s)| \right]
\end{equation}
where $\pi_i^k$ denotes the push-forward of $\pi_i$ to level $k$ via $\sigma_{i\to k}$.
\end{definition}

The window size of 10 levels provides sufficient resolution while maintaining computational tractability given entropic scaling.

\subsection{Cross-level metric}

\begin{definition}[Cross-Level Metric]
The cross-level distance between machines is:
\begin{equation}
d(M_i, M_j) = \Beh(i,j) + \lambda \cdot |I(i) - I(j)|
\end{equation}
where $I(n) = \kappa n \log n$ and $\lambda > 0$ weights capacity difference.
\end{definition}

\begin{theorem}[Metric Properties with Entropic Scaling]
The function $d$ defines a metric on $\{M_n\}$:
\begin{enumerate}
\item Non-negativity: $d(M_i, M_j) \geq 0$
\item Identity: $d(M_i, M_j) = 0 \iff i = j$
\item Symmetry: $d(M_i, M_j) = d(M_j, M_i)$
\item Triangle inequality: $d(M_i, M_k) \leq d(M_i, M_j) + d(M_j, M_k)$
\end{enumerate}
\end{theorem}

\begin{proof}
Properties (1)--(3) are immediate. For (4), the behavioral component satisfies triangle inequality by construction, and the capacity term $|I(i) - I(j)| = |\kappa i \log i - \kappa j \log j|$ satisfies it as an $L^1$ norm. Their weighted sum preserves the property.
\end{proof}

\subsection{Computational complexity}

\begin{proposition}[Complexity with Entropic Scaling]
Computing $\Beh(i,j)$ requires time:
\begin{equation}
O(|K| \cdot |D^k_{ij}|) = O(10 \cdot \exp(\kappa \max(i,j) \log \max(i,j)))
\end{equation}
where the exponential reflects the state space size at the witness level.
\end{proposition}

While the state space is exponential in information capacity, the entropic scaling $I(n) = \kappa n \log n$ ensures this remains tractable for realistic cognitive levels.

\section{Adjunction Structure}
\label{sec:adjunction}

\subsection{Projection and collapse operators}

\begin{definition}[Projection with Entropic Compression]
For $i < j$, the projection $P_{j\to i}: M_j \to M_i$ minimizes information loss:
\begin{equation}
P_{j\to i} = \arg\min_{\phi: S_j \to S_i} H(\phi(X_j) | X_j)
\end{equation}
subject to dynamics preservation where feasible. The information compressed is $I(j) - I(i) = \kappa(j \log j - i \log i)$ bits.
\end{definition}

\begin{definition}[Collapse with Entropic Reconstruction]
For $i < j$, the collapse $C_{i\to j}: M_i \to M_j$ optimally reconstructs higher-level structure:
\begin{equation}
C_{i\to j} = \arg\min_{\psi: S_i \to S_j} \mathbb{E}[\|X_j - \psi(P_{j\to i}(X_j))\|^2]
\end{equation}
This reconstructs the $\kappa(j \log j - i \log i)$ bits of missing information.
\end{definition}

\subsection{Adjunction theorem}

\begin{theorem}[Adjunction with Entropic Scaling]
\label{thm:adjunction}
The projection and collapse operators form an adjunction $(C \dashv P)$ in $\Fsm$:
\begin{equation}
\Hom_{\Fsm}(M_i, P_{j\to k}(M_j)) \cong \Hom_{\Fsm}(C_{i\to k}(M_i), M_j)
\end{equation}
for all $i \leq k < j$ in the hierarchy with entropic information scaling.
\end{theorem}

\begin{proof}[Proof sketch]
We construct the unit $\eta: \id \Rightarrow C \circ P$ and counit $\varepsilon: P \circ C \Rightarrow \id$ satisfying the triangle identities. The entropic scaling ensures these natural transformations are well-defined with bounded approximation error $\varepsilon_n \sim 1/(n \log n)$.
\end{proof}

\subsection{Adjunction error with entropic scaling}

\begin{proposition}[Approximation Error]
The adjunction is approximate with error:
\begin{equation}
\varepsilon_n = \|P \circ C - \id\| \sim \frac{1}{n \log n}
\end{equation}
This entropic decay ensures rapid convergence to exact adjunction as $n$ increases.
\end{proposition}

\section{Level Assignment Algorithm}
\label{sec:algorithm}

\subsection{Effective dimension with entropic scaling}

\begin{definition}[Entropic Effective Dimension]
For machine $M$ with state distribution $\pi$:
\begin{equation}
d_{\text{eff}}(M) = \frac{H(\pi)}{\log(\kappa n \log n)} \approx \frac{\text{actual entropy}}{\text{maximum entropic capacity}}
\end{equation}
This measures the fraction of entropic capacity utilized.
\end{definition}

\subsection{Assignment algorithm}

\begin{algorithm}[H]
\caption{Level Assignment with Entropic Scaling}
\begin{algorithmic}
\STATE \textbf{Input:} Machine $M = (S, f, \pi)$
\STATE \textbf{Output:} Hierarchy level $n^*$
\STATE
\STATE Compute $H(\pi) = -\sum_s \pi(s) \log \pi(s)$
\STATE Compute $\PR(\pi) = 1/\sum_s \pi(s)^2$
\STATE Find $n^* = \arg\min_n |I(n) - H(\pi)|$ where $I(n) = \kappa n \log n$
\STATE \textbf{return} $n^*$
\end{algorithmic}
\end{algorithm}

\begin{theorem}[Algorithm Correctness]
The algorithm assigns level $n^*$ that minimizes information capacity mismatch in time $O(|S| \log |S|)$.
\end{theorem}

\section{Extensions and Applications}
\label{sec:extensions}

\subsection{Behavioral distance decay model with entropic scaling}

\begin{proposition}[Entropic decay hypothesis]
For models separated by $\Delta n = |n_i - n_j|$ levels, behavioral distance decays as:
\begin{equation}
\Beh(n_i, n_j) \approx B_0 \exp(-\lambda \Delta I)
\end{equation}
where $\Delta I = |I(n_i) - I(n_j)| = |\kappa n_i \log n_i - \kappa n_j \log n_j|$ is the information capacity difference.
\end{proposition}

\textbf{Interpretation:} Distant levels become behaviorally similar as fine-grained differences wash out, with decay rate determined by entropic capacity gaps.

\subsection{Scaling law connection}

Classical scaling laws often assume exponential growth. With entropic scaling:
\begin{equation}
I(n) = \kappa n \log n \implies L(n) \sim \exp(-\alpha I(n)/\kappa)
\end{equation}

This provides a more realistic scaling relationship that avoids exponential resource requirements while maintaining power-law-like behavior over practical ranges.

\section{Discussion and Prior Work}
\label{sec:discussion}

\subsection{Relation to existing frameworks}

\begin{itemize}
\item \textbf{Kolmogorov complexity:} Our entropic scaling aligns with expected description lengths
\item \textbf{Rate-distortion theory:} The projection-collapse pair implements optimal compression-reconstruction with entropic bounds
\item \textbf{Cognitive architectures:} The hierarchy models resource-bounded reasoning with realistic scaling
\item \textbf{Neural networks:} Layer depth correlates with hierarchy level; entropic capacity matches biological constraints
\end{itemize}

\subsection{Advantages of entropic scaling}

The $I(n) = \kappa n \log n$ scaling provides:
\begin{enumerate}
\item \textbf{Biological plausibility:} Matches neural information processing limits
\item \textbf{Computational tractability:} Avoids exponential explosion
\item \textbf{Theoretical elegance:} Connects to Shannon entropy and thermodynamics
\item \textbf{Empirical alignment:} Consistent with observed cognitive capacities
\end{enumerate}

\section{Conclusion}
\label{sec:conclusion}

We have developed a complete mathematical framework for hierarchical computation with entropic scaling:

\begin{itemize}
\item \textbf{Metric structure:} Cross-level behavioral distance with entropic weighting
\item \textbf{Categorical structure:} Adjunction $(C \dashv P)$ with $1/(n \log n)$ error
\item \textbf{Algorithmic structure:} Polynomial-time level assignment
\item \textbf{Entropic scaling:} Information capacity $I(n) = \kappa n \log n$ throughout
\end{itemize}

The entropic scaling transforms the framework from a theoretical curiosity requiring exponential resources to a practical tool for understanding hierarchical computation in realistic systems. This opens new avenues for:

\begin{enumerate}
\item Analyzing biological neural hierarchies
\item Designing efficient AI architectures
\item Understanding consciousness and cognitive capacity
\item Optimizing information processing systems
\end{enumerate}

Future work includes empirical validation, extensions to continuous hierarchies, and applications to specific cognitive domains.

\begin{thebibliography}{10}

\bibitem{kowalczyk2025}
Kowalczyk, K. (2025).
\newblock Consciousness as Collapsed Computational Time: A Unified Theory with Entropic Scaling.
\newblock \emph{Zenodo}, doi:10.5281/zenodo.17556941.

\bibitem{shannon1948}
Shannon, C. E. (1948).
\newblock A mathematical theory of information.
\newblock \emph{Bell System Technical Journal}, 27(3), 379--423.

\bibitem{kolmogorov1965}
Kolmogorov, A. N. (1965).
\newblock Three approaches to the definition of the concept "quantity of information".
\newblock \emph{Problems of Information Transmission}, 1(1), 1--7.

\end{thebibliography}

\end{document}

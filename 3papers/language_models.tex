\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{algorithm}
\usepackage{algpseudocode}

\geometry{margin=1in}

% Theorem environments
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{prediction}[theorem]{Prediction}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{limitation}[theorem]{Limitation}

% Custom commands
\newcommand{\Fsm}{\textbf{Fsm}}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\id}{\text{id}}
\newcommand{\Beh}{\text{Beh}}

\title{Language Models as Hierarchical Computational Projections:\\A Theoretical Framework with Empirical Predictions}
\author{Karol Kowalczyk}
\date{November 9, 2025}

\begin{document}

\maketitle

\begin{abstract}
Large language models (LLMs) exhibit strikingly regular scaling patterns linking computational resources to capability, yet existing theories fail to explain why qualitative changes in reasoning arise only beyond certain model sizes. This paper situates LLMs within a formal hierarchy of finite computational systems, extending the \emph{Adjoint Projections on Computational Hierarchies} framework (Kowalczyk, 2025). We propose that each model functions as a finite machine $L_n = (S_n, f_n, \pi_n)$ at hierarchy level $n_L = \lfloor\alpha \log_2 P + \beta \log_2 V + \gamma\rfloor$, where $P$ is parameter count and $V$ is vocabulary size. Distillation and fine-tuning instantiate the adjunction's projection ($P$) and collapse ($C$) operators, governing information compression and expansion. We derive testable predictions and propose empirical validation protocols: (1) distillation energy should scale as $E \propto k_B T \Delta H$ plus architecture-dependent overhead, (2) emergent abilities should cluster near critical levels $n \approx 33$--35, and (3) behavioral distance should predict performance degradation under compression. Experimental verification of these predictions is future work. We outline detailed protocols to validate predictions across model families and discuss implications for scaling law theory, computational efficiency, and the theoretical foundations of artificial intelligence.
\end{abstract}

\noindent\textbf{Status:} This paper presents a theoretical framework with proposed empirical validation protocols. Experimental verification is future work.

\tableofcontents

\newpage

\section{Glossary of Symbols}
\label{sec:glossary}

For ease of reference, we collect the main notation used throughout:

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Symbol} & \textbf{Meaning} \\
\hline
$L_n$ & Language model at hierarchy level $n$ \\
$P$ & Parameter count of a language model \\
$V$ & Vocabulary size \\
$n_L$ & Hierarchy level: $\lfloor\alpha \log_2 P + \beta \log_2 V + \gamma\rfloor$ \\
$\alpha, \beta, \gamma$ & Coefficients in level assignment formula \\
$\Delta H$ & Information loss in bits (entropy difference) \\
$E_{\text{distill}}$ & Energy consumed during distillation \\
$\kappa$ & Architecture-dependent inefficiency factor \\
$k_B$ & Boltzmann constant ($1.38 \times 10^{-23}$ J/K) \\
$T$ & Temperature ($\approx$300 K for room temperature) \\
$\Beh(i,j)$ & Behavioral distance between levels $i$ and $j$ \\
$\lambda$ & Decay rate in behavioral distance model \\
$P_{j\to i}$ & Projection from level $j$ to level $i$ (distillation) \\
$C_{i\to j}$ & Collapse from level $i$ to level $j$ (fine-tuning) \\
$n_c$ & Critical hierarchy level for emergence \\
$\Fsm$ & Category of finite state machines \\
\hline
\end{tabular}
\end{center}

\section{Introduction}

\subsection{Scaling and the puzzle of emergence}

Large language model development follows empirical scaling laws where performance improves according to power relationships with model size. \cite{kaplan2020} showed that cross-entropy loss $L$ scales approximately as $L \propto N^{-\alpha}$ where $N$ represents parameters or training tokens, with $\alpha \approx 0.05$--0.1. \cite{hoffmann2022} refined these relationships through compute-optimal training schedules.

However, smooth scaling curves conceal a deeper phenomenon. \cite{wei2022} and \cite{ganguli2022} documented that new capabilities---multi-step reasoning, mathematical problem-solving, self-consistent planning---emerge discontinuously. A 1.3B-parameter model may completely fail tasks that a 6.7B-parameter model solves reliably. These \emph{emergent abilities} suggest structural reorganizations analogous to phase transitions in physical systems.

\textbf{The fundamental question:} What determines when and why these discontinuous transitions occur? Standard scaling laws offer no answer. We need a structural theory of computation accounting for how representational capacity and information flow scale with model complexity.

\subsection{Hierarchical computation as a unifying principle}

We adopt a hierarchical perspective building on the \emph{Adjoint Projections on Computational Hierarchies} framework \cite{kowalczyk2025}. Computation is modeled as a nested sequence of finite machines $\{M_n\}$, each operating on state spaces of size $2^n$. Higher levels simulate lower ones through embeddings, while projections compress state spaces. The adjunction $C \dashv P$ expresses duality between expansion (collapse $C$) and compression (projection $P$).

\textbf{Key insight:} When mapped to LLMs, these abstract operations correspond naturally to concrete practices:
\begin{itemize}
\item \textbf{Projection $\approx$ Distillation:} Compressing a large teacher model into a smaller student
\item \textbf{Collapse $\approx$ Fine-tuning:} Expanding or enriching representations through additional training
\end{itemize}

This mapping transforms qualitative emergence into a quantitative hypothesis: emergent behavior occurs at critical points where projections between adjacent hierarchy levels become irreversibly lossy, forcing representational reorganization.

\subsection{Contributions}

This paper contributes:

\begin{enumerate}
\item \textbf{Theoretical mapping:} Explicit correspondence between LLM attributes (parameters, vocabulary) and hierarchy levels $n_L$, with derived level assignment formula

\item \textbf{Testable predictions:} Three quantitative predictions regarding energy costs, emergence thresholds, and performance degradation under compression

\item \textbf{Experimental protocols:} Detailed methodology for validating predictions across 20--30 models from diverse families

\item \textbf{Unification:} Integration of scaling laws, information theory, and thermodynamic cost into a coherent hierarchical framework
\end{enumerate}

\subsection{Limitations and scope}

\begin{limitation}[No empirical results]
This paper provides a theoretical framework with testable predictions but does not present experimental validation. The predictions remain to be tested empirically.
\end{limitation}

\begin{limitation}[Simplified model mapping]
\label{lim:simplified}
Our treatment of neural networks as finite state machines abstracts away continuous activations, gradient dynamics, and stochastic training processes. This simplification enables mathematical tractability but may miss important aspects of neural network behavior.
\end{limitation}

\begin{limitation}[Physical claims require validation]
\label{lim:physical}
The thermodynamic predictions (e.g., $E \propto k_B T \Delta H$) are derived from information-theoretic principles but GPU energy consumption involves many confounding factors (memory bandwidth, parallelization overhead, cooling requirements). The predicted relationship should be viewed as a theoretical baseline requiring empirical calibration.
\end{limitation}

\begin{limitation}[Baseline comparisons needed]
This framework should be compared systematically with existing neural scaling theories, including effective theories of overparameterized networks, neural tangent kernel analyses, and empirical scaling law models.
\end{limitation}

\subsection{Paper organization}

Section~\ref{sec:related} reviews related work on scaling, emergence, and information theory. Section~\ref{sec:framework} presents the theoretical framework connecting adjunction theory to neural networks. Section~\ref{sec:level} derives the level assignment formula and proposes validation methodology. Section~\ref{sec:distillation} analyzes distillation as projection with thermodynamic implications. Section~\ref{sec:predictions} derives scaling predictions and identifies critical transitions. Section~\ref{sec:discussion} discusses limitations and future work. Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related}

\subsection{Scaling laws and performance predictability}

Neural network scaling laws quantify performance-resource relationships. \cite{kaplan2020} and \cite{hoffmann2022} showed that loss decreases predictably with compute, enabling extrapolation from small to large models. This regularity suggests underlying invariants analogous to universal scaling in statistical mechanics. However, power laws describe \emph{continuous} improvement but fail to capture \emph{discrete} capability jumps.

\subsection{Emergent abilities and discontinuous transitions}

\cite{wei2022} documented abrupt capability transitions: arithmetic reasoning, logical inference, and complex planning appear suddenly at specific model sizes. \cite{schaeffer2023} argued some apparent discontinuities reflect measurement artifacts, though genuine phase changes remain. \cite{ganguli2022} hypothesized transitions correspond to representational phase changes when information per parameter exceeds critical thresholds. However, these analyses lack mathematical models connecting emergence to computability or information geometry. The hierarchical framework provides that missing structure.

\subsection{Compression, distillation, and knowledge transfer}

Knowledge distillation \cite{hinton2015} transfers knowledge from large teachers to small students by minimizing KL divergence between output distributions. This realizes projection mathematically: compressing high-dimensional representational manifolds into lower-dimensional approximations while preserving behavioral equivalence. Empirical work \cite{sanh2019,jiao2020} demonstrates large efficiency gains at the cost of reduced output diversity---measurable information loss $\Delta H$.

\subsection{Information-theoretic perspectives}

The information bottleneck theory \cite{tishby2015} conceptualizes learning as trading compression against relevance. Subsequent analyses \cite{saxe2019} refined this for layer-level information flow but not for transitions between distinct model families. Our framework extends the bottleneck concept to entire computational hierarchies with cross-level metrics.

\subsection{Computational complexity and descriptive hierarchy}

Theoretically, language models function as finite automata with parametric extension. Their complexity class grows with parameter count and token diversity. \cite{tyszkiewicz1998} analyzed analogous hierarchies for streaming computation with bounded passes; \cite{tyszkiewicz2004,tyszkiewicz2010} extended this to expressivity gaps using games and Kolmogorov complexity. We reinterpret these results for machine learning, where expressivity gaps manifest as capability thresholds.

\section{Framework: Language Models as Finite Machines}
\label{sec:framework}

\subsection{Categorical background}

We briefly recap the adjunction framework from \cite{kowalczyk2025}. The category $\Fsm$ has:
\begin{itemize}
\item \textbf{Objects:} Finite machines $M_n = (S_n, f_n, \pi_n)$ with state space $S_n$, transition function $f_n: S_n \to S_n$, and stationary distribution $\pi_n$
\item \textbf{Morphisms:} Transition-preserving maps $\phi: M_i \to M_j$ satisfying $\phi \circ f_i = f_j \circ \phi$
\end{itemize}

An adjunction $C \dashv P$ consists of:
\begin{itemize}
\item \textbf{Projection} $P: \Fsm_{j} \to \Fsm_{i}$ (surjective, entropy-minimizing)
\item \textbf{Collapse} $C: \Fsm_{i} \to \Fsm_{j}$ (injective, left adjoint to $P$)
\item \textbf{Unit} $\eta: \id \Rightarrow C \circ P$ and \textbf{counit} $\varepsilon: P \circ C \Rightarrow \id$ satisfying triangle identities
\end{itemize}

The behavioral distance $\Beh(i,j)$ is a pseudometric quantifying functional divergence between levels $i$ and $j$ via synchronized Hamming disagreement over a window of intermediate levels. See Kowalczyk (2025) for proofs of metric properties and computational complexity bounds.

\subsection{Language models as hierarchy members}

\begin{definition}[LLM as finite machine]
\label{def:llm_machine}
A language model with parameter count $P$ and vocabulary $V$ corresponds to a finite machine $L_n = (S_n, f_n, \pi_n)$ where:
\begin{itemize}
\item $S_n$ is the discretized hidden state space (size $\approx 2^n$)
\item $f_n$ is the next-token prediction function
\item $\pi_n$ is the equilibrium distribution over hidden states
\end{itemize}
\end{definition}

\textbf{Intuition:} A transformer with $P$ parameters operating over vocabulary $V$ has internal representational capacity proportional to $P \cdot \log V$. The effective number of distinguishable states determines hierarchy level $n$.

\subsection{Distillation as projection}

\begin{definition}[Distillation operator]
For teacher model $L_T$ at level $n_T$ and student model $L_S$ at level $n_S < n_T$, distillation is the projection $P_{n_T \to n_S}: L_T \to L_S$ defined by:
\begin{equation}
P_{n_T \to n_S} = \arg\min_{P} \mathbb{E}_{x \sim D}\left[D_{\text{KL}}\big(L_T(x) \| L_S(x)\big)\right]
\end{equation}
where $D$ is the training distribution and outputs are probability distributions over $V$.
\end{definition}

\textbf{Properties:}
\begin{itemize}
\item Projection is surjective (student covers all coarse-grained teacher states)
\item Minimizes conditional entropy: $\arg\min H(S_{n_S} \mid P(S_{n_T}))$
\item Information loss: $\Delta H = n_T - n_S$ bits per token
\end{itemize}

\subsection{Fine-tuning as collapse}

\begin{definition}[Fine-tuning operator]
For base model $L_B$ at level $n_B$ and fine-tuned model $L_F$ at level $n_F \geq n_B$, fine-tuning is the collapse $C_{n_B \to n_F}: L_B \to L_F$ obtained through gradient descent on task-specific data.
\end{definition}

\textbf{Properties:}
\begin{itemize}
\item Collapse is injective (preserves base model capabilities)
\item Section property: $P \circ C \approx \id$ (distilling fine-tuned model recovers base)
\item Risk minimization: minimizes task loss while maintaining representational fidelity
\end{itemize}

\subsection{Connection to Tyszkiewicz hierarchies}

The expressive-power gaps documented by Tyszkiewicz \& Vianu (1998) for streaming computations and Tyszkiewicz (2004, 2010) for query languages provide theoretical precedent. In their framework:
\begin{itemize}
\item Level $n$ corresponds to $n$-pass computation or $n$-quantifier queries
\item Projections restrict computational resources (fewer passes, simpler queries)
\item Behavioral distances measure expressivity via Ehrenfeucht-Fra\"iss\'e games
\end{itemize}

Our contribution is recognizing that LLMs instantiate this abstract hierarchy with parameter count and vocabulary determining the level, and that distillation/fine-tuning realize the projection/collapse adjunction concretely.

\section{Level Assignment and Empirical Validation}
\label{sec:level}

\subsection{Theoretical derivation}

\begin{proposition}[Level assignment formula]
\label{prop:level_assignment}
For a language model with $P$ parameters and vocabulary size $V$, the hierarchy level is:
\begin{equation}
n_L = \left\lfloor \alpha \log_2 P + \beta \log_2 V + \gamma \right\rfloor
\end{equation}
where $\alpha, \beta, \gamma$ are universal constants to be determined empirically.
\end{proposition}

\begin{proof}[Derivation]
The effective state space size $|S_n| \sim 2^n$ must capture:
\begin{enumerate}
\item \textbf{Representational capacity:} $P$ parameters provide $O(P \log P)$ bits of information storage
\item \textbf{Vocabulary entropy:} Operating over $V$ tokens adds $O(\log V)$ bits per position
\item \textbf{Architectural efficiency:} Transformer attention mechanisms enable $O(P \log V)$ effective states
\end{enumerate}
Balancing these yields $n \sim \alpha \log P + \beta \log V + \gamma$ where:
\begin{itemize}
\item $\alpha$ quantifies parameter utilization efficiency (expected: $0.8$--1.2)
\item $\beta$ quantifies vocabulary contribution (expected: $0.3$--0.7)
\item $\gamma$ is an architecture-dependent offset (expected: $-5$ to $5$)
\end{itemize}
\end{proof}

\subsection{Normalization and identifiability}

To ensure uniqueness, we normalize:
\begin{itemize}
\item Fix $\alpha = 1$ (parameters as primary capacity measure)
\item Estimate $\beta, \gamma$ from empirical data
\end{itemize}

Alternative normalization: fix reference model (e.g., GPT-2 small at $n_{\text{ref}} = 25$) and calibrate others relative to it.

\subsection{Falsifiability criteria}

The level assignment hypothesis is falsifiable if:
\begin{enumerate}
\item Coefficients $\alpha, \beta, \gamma$ vary significantly across model families (GPT vs LLaMA vs BERT)
\item Formula fails to predict distillation fidelity (high $\Delta n$ but low KL divergence)
\item Emergence thresholds do not align with predicted critical levels
\end{enumerate}

\subsection{Validation methodology}

\subsubsection{Dataset construction}

Collect specifications for 30+ models:
\begin{itemize}
\item GPT family: GPT-2 (117M--1.5B), GPT-3 (125M--175B)
\item LLaMA family: LLaMA (7B--65B), LLaMA-2 (7B--70B)
\item OPT family: OPT (125M--175B)
\item BERT variants: BERT-base, BERT-large, RoBERTa, DistilBERT
\end{itemize}

For each model, record: $P$ (parameters), $V$ (vocabulary), $d$ (hidden dimension), $L$ (layers), $H$ (attention heads).

\subsubsection{Regression protocol}

\begin{enumerate}
\item \textbf{Feature engineering:} Compute $\log_2 P$, $\log_2 V$, and potential interaction terms
\item \textbf{Model fitting:} Ordinary least squares regression with leave-one-family-out cross-validation (train on GPT+LLaMA, test on OPT, etc.)
\item \textbf{Coefficient estimation:} Report $(\alpha, \beta, \gamma)$ with 95\% confidence intervals
\item \textbf{Prediction error:} Compute mean absolute error in predicted $n_L$ on held-out test set
\end{enumerate}

\textbf{Success criterion:} MAE $< 1.5$ levels (tolerating rounding errors) and coefficient stability ($\sigma_\alpha / \alpha < 0.15$) across cross-validation folds.

\subsubsection{Sensitivity analysis}

Assess robustness by:
\begin{itemize}
\item Varying training set composition (exclude entire families)
\item Testing on future models not in training set
\item Comparing against baseline models (e.g., $n_L = \log_2 P$ only)
\end{itemize}

\section{Distillation as Projection: Thermodynamic Analysis}
\label{sec:distillation}

\subsection{Information-theoretic characterization}

Distilling a teacher $L_T$ (level $n_T$) into student $L_S$ (level $n_S$) loses information:
\begin{equation}
\Delta H = n_T - n_S \text{ bits per token}
\end{equation}

This manifests as:
\begin{itemize}
\item Reduced output diversity: $H(L_S(x)) < H(L_T(x))$
\item Coarser hidden representations: fewer distinguishable internal states
\item Performance degradation on complex tasks requiring fine-grained distinctions
\end{itemize}

\subsection{Energy-entropy relationship}

\begin{prediction}[Distillation energy scaling]
\label{pred:energy}
The energy consumed during distillation scales as:
\begin{equation}
\label{eq:energy_distillation}
E_{\text{distill}} = \kappa \, k_B T \ln 2 \cdot \Delta H \cdot N_{\text{tokens}}
\end{equation}
where:
\begin{itemize}
\item $k_B = 1.38 \times 10^{-23}$ J/K is Boltzmann's constant
\item $T \approx 300$ K is operating temperature (room temperature)
\item $\Delta H$ is information loss measured in bits per token
\item $N_{\text{tokens}}$ is the total number of tokens processed during distillation
\item $\kappa \geq 1$ is an architecture-dependent inefficiency factor
\end{itemize}
\end{prediction}

\textbf{Units and scaling:} $\Delta H$ is measured in bits per token. For a distillation run processing $N_{\text{tokens}} = 10^9$ tokens with $\Delta H = 5$ bits/token, the theoretical Landauer bound gives:
\begin{equation}
E_{\text{Landauer}} = k_B T \ln 2 \cdot (5 \times 10^9) \approx 1.4 \times 10^{-11} \text{ J}
\end{equation}

In practice, GPU implementations have $\kappa \approx 10^{15}$--$10^{18}$ due to:
\begin{itemize}
\item Memory bandwidth overhead
\item Floating-point arithmetic inefficiencies
\item Parallelization costs
\item Heat dissipation requirements
\end{itemize}

Thus expected measured energies are $E_{\text{measured}} \approx 10^4$--$10^7$ J (order of kilowatt-hours), consistent with reported training costs.

\textbf{Dimensional consistency:} 
\begin{align*}
E_{\text{distill}} &= \kappa \cdot k_B T \ln 2 \cdot \Delta H \cdot N_{\text{tokens}} \\
&= [\text{dimensionless}] \times [\text{J/K}] \times [\text{K}] \times [\text{dimensionless}] \times [\text{bits}] \times [\text{tokens}] \\
&= \kappa \times 2.87 \times 10^{-21} \text{ J} \times (\Delta H \times N_{\text{tokens}}) \text{ bits}
\end{align*}

\subsection{Empirical measurement protocol}

\subsubsection{Controlled distillation experiments}

For each teacher-student pair $(L_T, L_S)$:

\begin{enumerate}
\item \textbf{Measure $\Delta H$:}
\begin{itemize}
\item Compute $H(L_T(x))$ and $H(L_S(x))$ via sampling over test set
\item Alternatively, use $\Delta H \approx n_T - n_S$ from level assignment
\end{itemize}

\item \textbf{Measure energy $E_{\text{distill}}$:}
\begin{itemize}
\item Use hardware power monitoring (NVIDIA SMI for GPUs)
\item Integrate power draw over distillation run: $E = \int P(t) \, dt$
\item Subtract baseline idle power to isolate computational cost
\item Uncertainty: $\pm 5\%$ from measurement noise and thermal fluctuations
\end{itemize}

\item \textbf{Estimate $\kappa$:}
\begin{equation}
\kappa = \frac{E_{\text{distill}}}{k_B T \ln 2 \cdot \Delta H \cdot N_{\text{tokens}}}
\end{equation}
\end{enumerate}

\subsubsection{Uncertainty propagation for $\kappa$}

Given measurement uncertainties:
\begin{itemize}
\item $\sigma_E / E \approx 0.05$ (5\% energy measurement error)
\item $\sigma_{\Delta H} / \Delta H \approx 0.10$ (10\% entropy estimation error)
\item $\sigma_T / T \approx 0.01$ (1\% temperature variation)
\end{itemize}

The combined uncertainty in $\kappa$ is:
\begin{equation}
\frac{\sigma_\kappa}{\kappa} = \sqrt{\left(\frac{\sigma_E}{E}\right)^2 + \left(\frac{\sigma_{\Delta H}}{\Delta H}\right)^2 + \left(\frac{\sigma_T}{T}\right)^2} \approx 0.11
\end{equation}

Thus we expect $\kappa$ determinations accurate to approximately $\pm 11\%$.

\subsubsection{Comparison across architectures}

Test hypothesis: $\kappa$ is universal across model families.

\begin{itemize}
\item Measure $\kappa$ for 10+ teacher-student pairs from GPT, LLaMA, OPT families
\item Test for significant variation: one-way ANOVA on $\log \kappa$ grouped by architecture
\item If $\kappa$ varies by $>2\times$ across families, theory requires refinement
\end{itemize}

\subsection{Behavioral distance and performance}

\begin{definition}[Behavioral distance for LLMs]
For models at levels $i$ and $j$, the behavioral distance is:
\begin{equation}
\Beh(i,j) \approx \frac{D_{\text{KL}}(L_i \| L_j)}{H(L_i)}
\end{equation}
where $D_{\text{KL}}$ is the KL divergence between output distributions and $H(L_i)$ is the output entropy of the higher-level model.
\end{definition}

\textbf{Justification:} This normalization ensures $0 \leq \Beh \leq 1$ and preserves the triangle inequality approximately for small divergences. For discrete distributions, the triangle inequality holds exactly when divergences are treated additively; our normalization by $H(L_i)$ converts absolute divergence to relative divergence, which remains approximately subadditive in typical neural network regimes where output distributions are smooth and overlapping.

\begin{prediction}[Distillation performance degradation]
\label{pred:performance}
For a student model $L_S$ distilled from teacher $L_T$, the performance gap on task $\mathcal{T}$ scales as:
\begin{equation}
\Delta \text{Acc}(\mathcal{T}) \propto \Beh(n_T, n_S)
\end{equation}
with proportionality constant depending on task complexity.
\end{prediction}

\textbf{Empirical test:}
\begin{enumerate}
\item Measure $\Beh(n_T, n_S)$ via KL divergence on held-out set
\item Evaluate both teacher and student on benchmark tasks (GLUE, SuperGLUE, reasoning benchmarks)
\item Compute accuracy gap: $\Delta \text{Acc} = \text{Acc}(L_T) - \text{Acc}(L_S)$
\item Fit linear model: $\Delta \text{Acc} = c_0 + c_1 \Beh(n_T, n_S)$
\item Test predictive power on held-out distillation pairs
\end{enumerate}

\section{Scaling Predictions and Critical Transitions}
\label{sec:predictions}

\subsection{Emergence and critical levels}

\begin{prediction}[Emergence threshold]
\label{pred:emergence}
Qualitative capability transitions occur near critical hierarchy levels $n_c \approx 33$--35, corresponding to models with approximately $P \approx 10^{10}$--$10^{11}$ parameters.
\end{prediction}

\textbf{Rationale (postulate pending validation):} Empirical observation suggests that tasks requiring multi-step reasoning and compositional generalization first appear reliably in models of this scale. This corresponds to the point where:
\begin{itemize}
\item State space size $2^{n_c} \approx 10^{10}$--$10^{11}$ exceeds typical task complexity
\item Projection fidelity $P \circ C \approx \id$ begins to fail systematically
\item Behavioral distance $\Beh(n_c-1, n_c+1)$ exhibits sharp increase
\end{itemize}

Future work should validate this threshold against empirical scaling curves from GPT-3, PaLM, and LLaMA benchmark suites.

\subsection{Behavioral distance decay model}

\begin{proposition}[Exponential decay hypothesis]
For models separated by $\Delta n = |n_i - n_j|$ levels, behavioral distance decays as:
\begin{equation}
\Beh(n_i, n_j) \approx B_0 e^{-\lambda \Delta n}
\end{equation}
where $\lambda$ is a decay rate (typical: $\lambda \approx 0.3$--0.5 per level) and $B_0$ is baseline divergence at adjacent levels.
\end{proposition}

\textbf{Interpretation:} Distant levels become behaviorally similar as fine-grained differences wash out. This predicts diminishing returns for scaling beyond certain thresholds.

\subsection{Scaling law connection}

Classical scaling laws \cite{kaplan2020}:
\begin{equation}
L(N) = \left(\frac{N_c}{N}\right)^\alpha
\end{equation}

Hierarchy perspective:
\begin{equation}
n_L \sim \log N \implies L(n) \sim e^{-\alpha n}
\end{equation}

Thus exponential loss decay in hierarchy levels corresponds to power-law scaling in parameter count, providing a structural interpretation of empirical scaling exponents.

\section{Discussion and Future Directions}
\label{sec:discussion}

\subsection{Theoretical strengths and limitations}

\textbf{Strengths:}
\begin{itemize}
\item Provides mathematical structure connecting information theory, thermodynamics, and computability
\item Generates precise, falsifiable predictions
\item Unifies distillation and fine-tuning as dual operations
\item Explains emergence via critical transitions in hierarchy
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
\item Finite-state abstraction omits gradient flow and continuous optimization
\item Thermodynamic predictions require careful experimental design to isolate effects
\item Critical levels ($n \approx 33$--35) are empirically motivated but not yet derived from first principles
\item Framework assumes parameter count and vocabulary are primary complexity measures, potentially underweighting architecture (depth, attention mechanisms)
\end{itemize}

\subsection{Comparison with alternative theories}

\begin{itemize}
\item \textbf{Neural tangent kernel:} Describes training dynamics in infinite-width limit; complementary to our finite-size hierarchy
\item \textbf{Information bottleneck:} Focuses on layer-wise compression; we extend to inter-model compression
\item \textbf{Lottery ticket hypothesis:} Concerns sparse subnetworks; orthogonal to hierarchy-level analysis
\item \textbf{Empirical scaling laws:} Phenomenological; our framework provides mechanistic interpretation
\end{itemize}

\subsection{Experimental validation roadmap}

To ensure falsifiability, we propose the following experimental validation protocols with pre-registered hypotheses and success criteria:

\subsubsection{Empirical validation}

Priority experiments:
\begin{enumerate}
\item \textbf{Level assignment validation:}
\begin{itemize}
\item Fit formula to 30+ models with leave-one-family-out cross-validation
\item Success criterion: MAE $< 1.5$ levels on held-out families
\item Report coefficients: $\alpha = 1.0 \pm 0.1$, $\beta = 0.5 \pm 0.15$, $\gamma = 0 \pm 3$
\end{itemize}

\item \textbf{Energy-entropy relationship:}
\begin{itemize}
\item Measure $E_{\text{distill}}$ for 10+ teacher-student pairs with varying $\Delta H$
\item Test linear relationship: $E = \kappa k_B T \ln 2 \cdot \Delta H \cdot N_{\text{tokens}}$
\item Success criterion: $R^2 > 0.80$ and $\kappa$ consistent within factor of $3\times$ across architectures
\end{itemize}

\item \textbf{Emergence threshold mapping:}
\begin{itemize}
\item Evaluate models from $n = 25$ to $n = 40$ on reasoning tasks (arithmetic, logic, code)
\item Identify critical levels where success rate jumps from $<20\%$ to $>60\%$
\item Test hypothesis: critical levels cluster in $n \in [33, 35]$ for multiple task families
\end{itemize}

\item \textbf{Behavioral distance-performance correlation:}
\begin{itemize}
\item Compute $\Beh(n_T, n_S)$ via KL divergence for 20+ distillation pairs
\item Measure accuracy gap $\Delta \text{Acc}$ on GLUE, SuperGLUE benchmarks
\item Test linear model: $\Delta \text{Acc} = c_0 + c_1 \Beh(n_T, n_S)$ with $R^2 > 0.70$
\end{itemize}
\end{enumerate}

\subsubsection{Theoretical extensions}

\begin{itemize}
\item \textbf{Continuous limit:} Replace discrete hierarchy with differential equations governing level transitions
\item \textbf{Multi-modal models:} Extend framework to vision-language models with cross-modal projections
\item \textbf{Training dynamics:} Incorporate gradient descent as trajectory through hierarchy levels
\item \textbf{Formal expressivity bounds:} Prove rigorous capacity limits at each hierarchy level using tools from descriptive complexity theory
\end{itemize}

\subsubsection{Practical applications}

\begin{itemize}
\item \textbf{Compression algorithms:} Design distillation protocols minimizing $\Beh(n_T, n_S)$ subject to efficiency constraints
\item \textbf{Architecture search:} Optimize network design for target hierarchy level given computational budget
\item \textbf{Capability prediction:} Estimate minimum model size for specific tasks from information-theoretic bounds on $\Delta H$
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

We have presented a theoretical framework situating large language models within a formal computational hierarchy. The key insights:

\begin{enumerate}
\item \textbf{Structural mapping:} LLMs instantiate finite machines at hierarchy levels determined by parameter count and vocabulary size

\item \textbf{Adjoint operations:} Distillation and fine-tuning realize projection/collapse operators with information-theoretic constraints

\item \textbf{Testable predictions:} Energy scaling, emergence thresholds, and compression-performance tradeoffs provide empirical falsification criteria

\item \textbf{Theoretical unification:} The framework connects scaling laws, information theory, and thermodynamics through categorical structure
\end{enumerate}

The framework's value lies not in immediate empirical confirmation but in providing \emph{precise quantitative predictions} that can be systematically tested. The specific threshold $n \approx 33$--35 for emergence, the energy-entropy relationship in Equation~\eqref{eq:energy_distillation}, and the behavioral distance-performance correlation are all falsifiable hypotheses.

Limitations are substantial: the finite-state abstraction is approximate, thermodynamic predictions require careful interpretation, and empirical validation is entirely absent. However, these limitations are clearly stated, and the framework provides sufficient mathematical structure to guide experimental design.

If validated, this approach could transform our understanding of model scaling from phenomenological observation to principled theory grounded in computation and information. If falsified, the systematic exploration will reveal which aspects of neural networks defy hierarchical description---equally valuable for scientific progress.

\begin{thebibliography}{10}

\bibitem{ganguli2022}
Ganguli, D., et al. (2022).
\newblock Predictability and surprise in large generative models.
\newblock In \emph{Proceedings of FAccT}.

\bibitem{hinton2015}
Hinton, G., Vinyals, O., \& Dean, J. (2015).
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}.

\bibitem{hoffmann2022}
Hoffmann, J., et al. (2022).
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}.

\bibitem{jiao2020}
Jiao, X., et al. (2020).
\newblock TinyBERT: Distilling BERT for natural language understanding.
\newblock In \emph{Proceedings of EMNLP}.

\bibitem{kaplan2020}
Kaplan, J., et al. (2020).
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}.

\bibitem{kowalczyk2025}
Kowalczyk, K. (2025).
\newblock Adjoint projections on computational hierarchies: A metric framework.
\newblock \emph{Manuscript in preparation}.

\bibitem{sanh2019}
Sanh, V., et al. (2019).
\newblock DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}.

\bibitem{saxe2019}
Saxe, A. M., et al. (2019).
\newblock On the information bottleneck theory of deep learning.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019(12), 124020.

\bibitem{schaeffer2023}
Schaeffer, R., Miranda, B., \& Koyejo, S. (2023).
\newblock Are emergent abilities of large language models a mirage?
\newblock \emph{arXiv preprint arXiv:2304.15004}.

\bibitem{tishby2015}
Tishby, N., \& Zaslavsky, N. (2015).
\newblock Deep learning and the information bottleneck principle.
\newblock In \emph{Proceedings of ITW}.

\bibitem{tyszkiewicz1998}
Tyszkiewicz, J., \& Vianu, V. (1998).
\newblock Queries and computation on the web.
\newblock In \emph{Proceedings of ICDT}, 275--289.

\bibitem{tyszkiewicz2004}
Tyszkiewicz, J. (2004).
\newblock On asymptotic probabilities of monadic second order properties.
\newblock In \emph{Proceedings of ICALP}, 887--899.

\bibitem{tyszkiewicz2010}
Tyszkiewicz, J. (2010).
\newblock Kolmogorov complexity and expressive power.
\newblock \emph{Information and Computation}, 208(7), 729--743.

\bibitem{wei2022}
Wei, J., et al. (2022).
\newblock Emergent abilities of large language models.
\newblock \emph{Transactions on Machine Learning Research}.

\end{thebibliography}

\end{document}
